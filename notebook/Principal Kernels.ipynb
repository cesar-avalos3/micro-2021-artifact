{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sys import argv\n",
    "import gzip\n",
    "import copy\n",
    "import json\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from ipywidgets import interact_manual, widgets, interactive\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import statistics\n",
    "import scipy\n",
    "import paths\n",
    "import RKS\n",
    "import RGS\n",
    "import pickle\n",
    "\n",
    "def save_variables(proc, filename):\n",
    "    with open(filename+'.pickle', 'wb') as f:\n",
    "        pickle.dump(proc, f)\n",
    "        \n",
    "def open_variables(filename):\n",
    "    with open(filename+'.pickle', 'rb') as f:\n",
    "        proc = pickle.load(f)\n",
    "    return proc\n",
    "\n",
    "# RKS --------------------------------------------------------------\n",
    "# Because the nsight compute program doesn't have an exclusive\n",
    "# export to CSV option, using the buffer stdout instead, we have to\n",
    "# get rid of some lines, this function does that. \n",
    "def ignore_lines(file):\n",
    "    # check the first 1000 lines, if the phrase \"device__attribute_async_engine_count\"\n",
    "    # is found, else return -99\n",
    "    # This phrase is always printed by nsight-compute.\n",
    "    i = 0\n",
    "    with open(file) as in_file:\n",
    "        for line in in_file:\n",
    "            if (\"device__attribute_async_engine_count\" in line):\n",
    "                return i\n",
    "            elif(i > 1000):\n",
    "                return -99\n",
    "            i += 1\n",
    "            \n",
    "# General open file function, takes in a nsight-compute csv filename, \n",
    "# returns a pandas table.\n",
    "def open_file(filename):\n",
    "    #print(filename)\n",
    "    lines_to_ignore = ignore_lines(filename)\n",
    "    if(lines_to_ignore == -99):\n",
    "        lines_to_ignore = 0\n",
    "    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)\n",
    "    if(table.loc[0,'ID'] != 0.0):\n",
    "        table = table.drop(0)\n",
    "        table.index = range(len(table))\n",
    "    # For certain systems nsight-compute prints\n",
    "    # comma-deliniated numbers, e.g. 1,000 instead of 1000.\n",
    "    # We need to get rid of that. \n",
    "    table = table.replace(',','', regex=True)\n",
    "    return table\n",
    "\n",
    "# List of agnostic features to take into consideration when performing\n",
    "# the PCA (i.e. only these variables are going to be used)\n",
    "agnostic_features = ['l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum',\n",
    "                     'l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum',\n",
    "                     #'l1tex__t_sectors_pipe_lsu_mem_local_op_ld.sum',\n",
    "                     'smsp__inst_executed.sum','smsp__thread_inst_executed_per_inst_executed.ratio',\n",
    "                     #'smsp__sass_inst_executed_op_global_atom.sum',\n",
    "                     'smsp__inst_executed_op_global_ld.sum',\n",
    "                     'smsp__inst_executed_op_global_st.sum',\n",
    "                     'smsp__inst_executed_op_shared_ld.sum',\n",
    "                     'smsp__inst_executed_op_shared_st.sum',\n",
    "                     #'smsp__inst_executed_op_surface_atom.sum',\n",
    "                     #'smsp__inst_executed_op_surface_ld.sum',\n",
    "                     #'smsp__inst_executed_op_surface_red.sum',\n",
    "                     #'smsp__inst_executed_op_surface_st.sum',\n",
    "                     'sass__inst_executed_global_loads',\n",
    "                     'sass__inst_executed_global_stores',\n",
    "                     'launch__grid_size']\n",
    "\n",
    "# Opens the cycles-profiled-only csvs generated by\n",
    "# the nsight-compute utility, obtained using the \n",
    "# run_hw.py under the accel-sim utility.\n",
    "def open_cycles(filename):\n",
    "    #print(filename)\n",
    "    lines_to_ignore = ignore_cycle_lines(filename)\n",
    "    if(lines_to_ignore == -99):\n",
    "        lines_to_ignore = 0\n",
    "    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)\n",
    "    return table\n",
    "\n",
    "def ignore_cycle_lines(file):\n",
    "    # check the first 1000 lines, if the phrase \"Section Name\" is not found\n",
    "    # return -99. Same as ignore_lines above, but for the cycles-only version.\n",
    "    i = 0\n",
    "    with open(file) as in_file:\n",
    "        for line in in_file:\n",
    "            if (\"Section Name\" in line):\n",
    "                return i\n",
    "            elif(i > 1000):\n",
    "                return -99\n",
    "            i += 1\n",
    "            \n",
    "# Generates Pandas DataFrame, normalizing data, correcting for NaNs, in some systems\n",
    "# Nsight Compute might return numbers with commas which are interpreted by pandas as strings\n",
    "# rather than numbers\n",
    "# Also concatenate Kernel Name, Cycles and Kernel ID to the table.\n",
    "# No PCA is performed\n",
    "def generate_DF(table, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID']):\n",
    "    x_large = table.loc[:,agnostic_features].values\n",
    "    x_large = StandardScaler().fit_transform(x_large)\n",
    "    temp = table.loc[:,agnostic_features]\n",
    "    # Check if there is any non-numbers \n",
    "    for value in agnostic_features:\n",
    "        if(temp[value].isnull().values.any()):\n",
    "            print(value)\n",
    "    for new_col in appendable_columns:\n",
    "        temp = table[[new_col]]\n",
    "        if \"Name\" not in new_col:\n",
    "            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))\n",
    "        try:\n",
    "            x_large = pd.concat([x_large, temp], axis = 1)\n",
    "        except:\n",
    "            x_large = pd.concat([pd.DataFrame(x_large), pd.DataFrame(temp)], axis = 1)\n",
    "    return x_large\n",
    "\n",
    "# Generates Pandas DataFrame, normalizing data, correcting for NaNs, in some systems\n",
    "# Nsight Compute might return numbers with commas which are interpreted by pandas as strings\n",
    "# rather than numbers\n",
    "# Also concatenate Kernel Name, Cycles and Kernel ID to the table. \n",
    "# Same as above but performs PCA\n",
    "def generate_PCA(table, pca_variation=0.9995, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID'], debug=False):\n",
    "    x_large = table.loc[:,agnostic_features].values\n",
    "    #x_large = x_large.apply(lambda x: pd.to_numeric(x.astype(str).str.replace(',','')))\n",
    "    #x_large = x_large.values\n",
    "    x_large = StandardScaler().fit_transform(x_large)\n",
    "    temp = table.loc[:,agnostic_features]\n",
    "    for value in agnostic_features:\n",
    "        if(temp[value].isnull().values.any()):\n",
    "            print(value)\n",
    "    pca_components = pca_variation\n",
    "    pca = PCA(pca_components)\n",
    "    principalComponents_Large = pca.fit_transform(x_large)\n",
    "    principalDf_Large = pd.DataFrame(data = principalComponents_Large, columns = ['principal component '+str(x+1) for x in range(pca.n_components_)])\n",
    "    for new_col in appendable_columns:\n",
    "        temp = table[[new_col]]\n",
    "        if \"Name\" not in new_col:\n",
    "            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))\n",
    "        principalDf_Large = pd.concat([principalDf_Large, temp], axis = 1)\n",
    "    #finalDf_Large = pd.concat([finalDf_Large,truncated_names], axis = 1)\n",
    "    #finalDf_Large = pd.concat([finalDf_Large, nsight_csv[['ID']]], axis = 1)\n",
    "    #finalDf_sorted = finalDf_Large#.sort_values('sm__inst_issued.avg.per_cycle_active [inst/cycle]')\n",
    "    #%matplotlib inline\n",
    "    #fig = plt.figure(figsize = (10,10))\n",
    "    #ax = fig.add_subplot(111) \n",
    "    #ax.set_title('PCA components - variance', fontsize = 20)\n",
    "    #plt.bar(range(pca.n_components_), pca.explained_variance_ratio_, color='black')\n",
    "    #plt.xlabel('PCA features')\n",
    "    #plt.ylabel('variance %')\n",
    "    #plt.xticks(range(pca.n_components_))\n",
    "    #plt.show()\n",
    "    # Clustering\n",
    "    #k_means = KMeans(n_clusters=2, random_state=2).fit(principalComponents_Large)\n",
    "    #finalDf_sorted['Segments'] = k_means.labels_\n",
    "    #print(principalDf_Large)\n",
    "    return principalDf_Large, principalComponents_Large \n",
    "\n",
    "# Cluster PCA and Dataframe returned from PCA method above\n",
    "# Runs Kmeans, sweeping the number of clusters, starting_from and ending_at control the specific cluster sweep values.\n",
    "# Column_variable is the variable by which we calculate our error by.\n",
    "\n",
    "def kmeans_clustering(dataFrame,principalComponents_dataFrame,starting_from=1, ending_at=20, column_variable='gpc__cycles_elapsed.avg',print_output=False):\n",
    "    total_runtime = dataFrame[column_variable].sum()\n",
    "    complete_groups = []\n",
    "    random_per_K = []\n",
    "    if(ending_at > len(dataFrame)):\n",
    "        ending_at = len(dataFrame)+1\n",
    "    results = {'random_choices_projections':[], 'random_choices_vectors':[], 'random_choices_names':[], \n",
    "               'random_choices_id':[], 'random_choices_speedups':[],'first_choices_projections':[],\n",
    "               'center_choices_projections':[], 'center_choices_vectors':[], 'center_choices_names':[],\n",
    "               'center_choices_id':[], 'center_choices_speedups':[], 'center_choices_errors': [],\n",
    "               'complete_groups':[],\n",
    "               'first_choices_vectors':[], 'first_choices_names':[], 'first_choices_id':[], 'group_count':[], \n",
    "               'total_runtime':total_runtime, 'group_number': [], 'errors': [], 'speedups': [], 'number_of_kernels': []}\n",
    "    for i in range(starting_from, ending_at):\n",
    "        random_choices = [] # Randomly select a kernel from the group\n",
    "        random_choices_name = []\n",
    "        random_choices_id = []\n",
    "        mean_choices = []   # Select the kernel with the value closest to the mean\n",
    "        max_choices = []    # Select the largest kernel value\n",
    "        first_choices = []   # Selects the first chronological value\n",
    "        first_choices_name = []\n",
    "        first_choices_id = []\n",
    "        center_choices_id = []\n",
    "        closest_to_mean = []\n",
    "        closest_to_mean_names = []\n",
    "        group_count = []    # Number of elements inside the cluster i\n",
    "        complete_groups_df = []\n",
    "        k_means = KMeans(n_clusters=i, random_state=4).fit(principalComponents_dataFrame)\n",
    "        dataFrame['Segments'] = k_means.labels_\n",
    "        #center_ids = cluster_centers(dataFrame,principalComponents_dataFrame,k_means)\n",
    "        per_group_random = []\n",
    "        for group in np.unique(k_means.labels_):\n",
    "            temp_df = dataFrame.loc[dataFrame['Segments'] == group]\n",
    "            #closest_to_mean.append(temp_df.loc[center_ids[group], column_variable])\n",
    "            #closest_to_mean_names.append(temp_df.loc[center_ids[group], 'Kernel Name'])\n",
    "            complete_groups_df.append(temp_df)\n",
    "            temp_df.index = range(len(temp_df))\n",
    "            #value_first = temp_df.loc[0,column_variable]\n",
    "            first_choices.append(temp_df.loc[0,column_variable])\n",
    "            first_choices_name.append(temp_df.loc[0,'Kernel Name'])\n",
    "            first_choices_id.append(temp_df.loc[0,'ID'])\n",
    "            #center_choices_id.append(center_ids[group])\n",
    "            temp_df_sorted = temp_df.sort_values(column_variable)\n",
    "            temp_df_sorted.index = range(len(temp_df_sorted))\n",
    "            group_count.append(len(temp_df))\n",
    "            random_vals = []\n",
    "            random_names = []\n",
    "            random_ids = []\n",
    "            random_choice = random.randint(0,len(temp_df)-1)\n",
    "            random_choice_name = ''\n",
    "            for i in range(10):\n",
    "                random_choice_ = random.randint(0,len(temp_df))\n",
    "                random_choice_name = ''\n",
    "                try:\n",
    "                    random_vals.append(temp_df_sorted.loc[random_choice_,column_variable] * len(temp_df))\n",
    "                    random_names.append(temp_df_sorted.loc[random_choice, 'Kernel Name'])\n",
    "                    random_ids.append(random_choice_)\n",
    "                except:\n",
    "                    random_vals.append(temp_df_sorted.loc[0,column_variable] * len(temp_df))\n",
    "                    random_names.append(temp_df_sorted.loc[0, 'Kernel Name'])\n",
    "                    random_ids.append(random_choice)\n",
    "            max_choice = temp_df_sorted.loc[len(temp_df)-1, column_variable]\n",
    "            try:\n",
    "                value_mean = temp_df_sorted.loc[int(len(temp_df)/2),column_variable]\n",
    "            except:\n",
    "                print(int(len(temp_df)/2))\n",
    "            try:\n",
    "                value_random = temp_df_sorted.loc[random_choice,column_variable]\n",
    "                random_choice_name = temp_df_sorted.loc[random_choice, 'Kernel Name']\n",
    "            except:\n",
    "                print(\"Why would this happen?\")\n",
    "                print(\"Random Choice: \"+str(random_choice))\n",
    "                print(\"Length dataFrame: \"+str(len(temp_df_sorted)))\n",
    "                value_random = temp_df_sorted.loc[0,column_variable]\n",
    "                random_choice_name = temp_df_sorted.loc[0,'Kernel Name']\n",
    "            #value_random = 0\n",
    "            random_choices.append(value_random)\n",
    "            random_choices_id.append(random_choice)\n",
    "            random_choices_name.append(random_choice_name)\n",
    "            mean_choices.append(value_mean)\n",
    "            max_choices.append(max_choice)\n",
    "            per_group_random.append({'random_vals': random_vals, 'random_names': random_names, 'random_ids': random_ids})\n",
    "        complete_groups.append(complete_groups_df)\n",
    "        random_runtime = [random_choices[i] * group_count[i] for i in range(len(random_choices))]\n",
    "        mean_runtime = [mean_choices[i] * group_count[i] for i in range(len(random_choices))]\n",
    "        max_runtime = [max_choices[i] * group_count[i] for i in range(len(random_choices))]\n",
    "        first_runtime = [first_choices[i] * group_count[i] for i in range(len(first_choices))]\n",
    "        #closest_runtime = [closest_to_mean[i] * group_count[i] for i in range(len(random_choices))]\n",
    "        random_per_K.append(per_group_random)\n",
    "        if(print_output):\n",
    "            print('For '+str(i)+ ' groups')\n",
    "            print('----------------------------------------------')\n",
    "            #print(dataFrame[column_variable])\n",
    "            print('The actual run time is: '+str(total_runtime))\n",
    "            print('----------------- Projections ----------------')\n",
    "            print(' ')\n",
    "            print('The random value run time is: '+str(np.sum(random_runtime)))\n",
    "            print('The error is '+ str(np.sum(random_runtime) / total_runtime ))\n",
    "            print('The mean value run time is: '+str(np.sum(mean_runtime)))\n",
    "            print('The error is '+ str(np.sum(mean_runtime) / total_runtime ))\n",
    "            print('The max value run time is: '+str(np.sum(max_runtime)))\n",
    "            print('The error is '+ str(np.sum(max_runtime) / total_runtime ))\n",
    "            print('The first value run time is: '+str(np.sum(first_runtime)))\n",
    "            print('The error is '+ str(np.sum(first_runtime) / total_runtime ))\n",
    "            print('The speedup is '+ str(total_runtime / np.sum(first_choices)))\n",
    "            print('The reduced number of kernels '+str(len(first_runtime)))\n",
    "            print('The total number of kernels '+str(len(dataFrame)))\n",
    "            print('The first choice vector is '+str(first_choices))\n",
    "            print('The number of elements per group is '+str(group_count))\n",
    "            print('The names of the first choices are '+str(first_choices_name))\n",
    "            print('The kernel IDs of the first choices are '+str(first_choices_id))\n",
    "        #print('Their product is '+str( [first_choices[i] * group_count[i] for i in range(len(first_choices))] ))\n",
    "            print(' ')\n",
    "        results['errors'].append(np.abs((np.sum(first_runtime)-total_runtime)) / total_runtime)\n",
    "        results['speedups'].append(total_runtime / np.sum(first_choices))\n",
    "        results['first_choices_vectors'].append(first_choices)\n",
    "        results['first_choices_projections'].append(first_runtime)\n",
    "        results['first_choices_names'].append(first_choices_name)\n",
    "        results['first_choices_id'].append(first_choices_id)\n",
    "        results['random_choices_vectors'].append(random_choices)\n",
    "        results['random_choices_projections'].append(random_runtime)\n",
    "        results['random_choices_names'].append(random_choices_name)\n",
    "        results['random_choices_id'].append(random_choices_id)\n",
    "        #results['center_choices_projections'].append(closest_runtime)\n",
    "        #results['center_choices_id'].append(center_choices_id)\n",
    "        #results['center_choices_vectors'].append(closest_to_mean)\n",
    "        #results['center_choices_names'].append(closest_to_mean_names)\n",
    "        #results['center_choices_errors'].append(np.abs((total_runtime - np.sum(closest_runtime))) / total_runtime)\n",
    "        results['group_count'].append(group_count)\n",
    "        results['group_number'].append(group)\n",
    "        results['complete_groups'].append(complete_groups)\n",
    "        results['number_of_kernels'].append(len(dataFrame))\n",
    "        results['random_per_K'] = random_per_K\n",
    "        #print(finalDf_sorted.loc[finalDf_sorted['Segments'] == group])\n",
    "    return results\n",
    "#kmeans_clustering(finalDf_sorted, principalComponents_Large)\n",
    "\n",
    "# Returns the ID of the element closest to the group's centers\n",
    "def cluster_centers(dataFrame, principalComponents_dataFrame, k_means, debug=False):\n",
    "    centers_vector = []\n",
    "    centers_ids = []\n",
    "    clusters_centers = k_means.cluster_centers_\n",
    "    temp_pca_df = dataFrame.copy()\n",
    "    #labels = ['principal component '+str(x+1) for x in range(len(np.unique(k_means.labels_)))]\n",
    "    for group in np.unique(k_means.labels_):\n",
    "        temp_df = dataFrame.loc[dataFrame['Segments'] == group]\n",
    "        temp_grouped_pca_df = temp_df.filter(regex='principal')\n",
    "        original_indeces = temp_grouped_pca_df.index.values.tolist()\n",
    "        temp_grouped_pca_df.index = range(len(temp_grouped_pca_df))\n",
    "        minimum_distance = temp_grouped_pca_df.loc[0,:].values.tolist()\n",
    "        minimum_index = original_indeces[0]\n",
    "        #print(minimum_distance)\n",
    "        if(debug):\n",
    "            print('The clusters centers are: '+str(clusters_centers[group]))\n",
    "            print('The first data point is : '+str(minimum_distance))\n",
    "        minimum_distance = distance.euclidean(minimum_distance, clusters_centers[group])\n",
    "        if(debug):\n",
    "            print(minimum_distance)\n",
    "        for i in range(len(temp_grouped_pca_df)):\n",
    "            data_point = temp_grouped_pca_df.loc[i,:].values.tolist()\n",
    "            temp_distance = distance.euclidean(data_point, clusters_centers[group])\n",
    "            if(temp_distance < minimum_distance):\n",
    "                minimum_distance = temp_distance\n",
    "                minimum_index = original_indeces[i]\n",
    "        if(debug):\n",
    "            print(minimum_distance, minimum_index)\n",
    "        centers_ids.append(minimum_index)\n",
    "    if(debug):\n",
    "        print('Returns from function \\nNew iteration\\n\\n')\n",
    "    return centers_ids\n",
    "\n",
    "def point_closest_to_all(dataFrame, principalComponents_dataFrame, k_means, debug=False):\n",
    "    closest_ids = []\n",
    "    for group in np.unique(k_means.labels_):\n",
    "        temp_df = dataFrame.loc[dataFrame['Segments'] == group]\n",
    "        temp_grouped_pca_df = temp_df.filter(regex='principal')\n",
    "        original_indeces = temp_grouped_pca_df.index.value.tolist()\n",
    "    \n",
    "def check_results(dataFrame, best_choices, group_counts, column_variable='gpc__cycles_elapsed.avg',print_debug=True):\n",
    "    result_dictionary = {}\n",
    "    total_runtime = pd.to_numeric(dataFrame[column_variable]).sum()\n",
    "    runtime_list = []\n",
    "    projected_runtime = 0.0\n",
    "    for i in range(len(best_choices)):\n",
    "        index = int(best_choices[i])\n",
    "        runtime_list.append(float(dataFrame.loc[index, column_variable]))\n",
    "        projected_runtime += float(dataFrame.loc[index, column_variable]) * group_counts[i]\n",
    "    if(print_debug):\n",
    "        print('The actual runtime is: '+str(total_runtime))\n",
    "        print('The projected runtime with the borrowed K-Means is: '+str(projected_runtime))\n",
    "        print('The ratio is '+str(projected_runtime/total_runtime))\n",
    "    result_dictionary = {'total_runtime': total_runtime, 'projected_runtime': projected_runtime,\n",
    "                         'error': np.abs(projected_runtime - total_runtime)/total_runtime,\n",
    "                         'speedup': total_runtime / np.sum(runtime_list), 'reduced_runtime': np.sum(runtime_list), \n",
    "                         'runtime_list': runtime_list}\n",
    "    return result_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open Nsight Compute CSV\n",
    "table = open_file(\"tiny_test.csv\")\n",
    "# Apply PCA\n",
    "# We can adjust the number of principal components with the pca_variation argument\n",
    "df, pca = generate_PCA(table)\n",
    "#results = kmeans_clustering(df, pca,print_output = True)\n",
    "#By default the number of clusters sweeped goes from 1 cluster to 20.\n",
    "results = kmeans_clustering(df, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1 clusters \n",
      "Principal Kernel: [0.0]\n",
      "Group Count: [34]\n",
      "For 2 clusters \n",
      "Principal Kernel: [1.0, 0.0]\n",
      "Group Count: [32, 2]\n",
      "For 3 clusters \n",
      "Principal Kernel: [1.0, 0.0, 12.0]\n",
      "Group Count: [31, 2, 1]\n",
      "For 4 clusters \n",
      "Principal Kernel: [6.0, 0.0, 12.0, 1.0]\n",
      "Group Count: [26, 2, 1, 5]\n",
      "For 5 clusters \n",
      "Principal Kernel: [6.0, 4.0, 0.0, 12.0, 2.0]\n",
      "Group Count: [25, 1, 2, 1, 5]\n",
      "For 6 clusters \n",
      "Principal Kernel: [1.0, 6.0, 4.0, 12.0, 8.0, 0.0]\n",
      "Group Count: [4, 25, 1, 1, 2, 1]\n",
      "For 7 clusters \n",
      "Principal Kernel: [1.0, 6.0, 4.0, 12.0, 8.0, 0.0, 26.0]\n",
      "Group Count: [4, 23, 1, 1, 2, 1, 2]\n",
      "For 8 clusters \n",
      "Principal Kernel: [2.0, 9.0, 4.0, 12.0, 1.0, 0.0, 8.0, 26.0]\n",
      "Group Count: [5, 21, 1, 1, 1, 1, 2, 2]\n",
      "For 9 clusters \n",
      "Principal Kernel: [13.0, 4.0, 12.0, 2.0, 0.0, 26.0, 8.0, 6.0, 1.0]\n",
      "Group Count: [16, 1, 1, 3, 1, 2, 2, 7, 1]\n",
      "For 10 clusters \n",
      "Principal Kernel: [13.0, 0.0, 12.0, 5.0, 4.0, 1.0, 8.0, 2.0, 26.0, 9.0]\n",
      "Group Count: [16, 1, 1, 3, 1, 1, 2, 2, 2, 5]\n",
      "For 11 clusters \n",
      "Principal Kernel: [13.0, 4.0, 12.0, 2.0, 0.0, 26.0, 10.0, 9.0, 1.0, 8.0, 5.0]\n",
      "Group Count: [16, 1, 1, 2, 1, 2, 1, 5, 1, 1, 3]\n",
      "For 12 clusters \n",
      "Principal Kernel: [13.0, 4.0, 12.0, 3.0, 0.0, 26.0, 10.0, 9.0, 1.0, 8.0, 5.0, 2.0]\n",
      "Group Count: [16, 1, 1, 1, 1, 2, 1, 5, 1, 1, 3, 1]\n",
      "For 13 clusters \n",
      "Principal Kernel: [9.0, 0.0, 12.0, 4.0, 3.0, 8.0, 1.0, 26.0, 10.0, 5.0, 2.0, 14.0, 13.0]\n",
      "Group Count: [5, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 15, 1]\n",
      "For 14 clusters \n",
      "Principal Kernel: [5.0, 14.0, 4.0, 12.0, 1.0, 0.0, 8.0, 26.0, 2.0, 9.0, 10.0, 6.0, 3.0, 13.0]\n",
      "Group Count: [1, 15, 1, 1, 1, 1, 1, 2, 1, 5, 1, 2, 1, 1]\n",
      "For 15 clusters \n",
      "Principal Kernel: [23.0, 4.0, 12.0, 1.0, 6.0, 0.0, 8.0, 26.0, 2.0, 9.0, 3.0, 10.0, 5.0, 13.0, 14.0]\n",
      "Group Count: [7, 1, 1, 1, 2, 1, 1, 2, 1, 5, 1, 1, 1, 1, 8]\n",
      "For 16 clusters \n",
      "Principal Kernel: [14.0, 4.0, 3.0, 0.0, 12.0, 8.0, 1.0, 26.0, 9.0, 10.0, 2.0, 7.0, 13.0, 5.0, 6.0, 25.0]\n",
      "Group Count: [9, 1, 1, 1, 1, 1, 1, 2, 5, 1, 1, 1, 1, 1, 1, 6]\n",
      "For 17 clusters \n",
      "Principal Kernel: [25.0, 4.0, 12.0, 3.0, 0.0, 26.0, 10.0, 14.0, 1.0, 8.0, 6.0, 2.0, 9.0, 5.0, 7.0, 13.0, 16.0]\n",
      "Group Count: [3, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 5, 1, 1, 1, 8]\n",
      "For 18 clusters \n",
      "Principal Kernel: [16.0, 0.0, 12.0, 6.0, 4.0, 1.0, 8.0, 3.0, 26.0, 9.0, 10.0, 2.0, 5.0, 13.0, 7.0, 25.0, 14.0, 21.0]\n",
      "Group Count: [8, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 3, 4, 2]\n",
      "For 19 clusters \n",
      "Principal Kernel: [28.0, 4.0, 12.0, 1.0, 6.0, 0.0, 8.0, 26.0, 2.0, 21.0, 3.0, 10.0, 5.0, 13.0, 14.0, 9.0, 7.0, 25.0, 16.0]\n",
      "Group Count: [2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 4, 3, 1, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "# This prints the groups compositon and group weights/counts \n",
    "for i,(x,y) in enumerate(zip(results['first_choices_id'], results['group_count'])):\n",
    "    print(\"For \" + str(i+1) +\" clusters \")\n",
    "    print(\"Principal Kernel: \" + str(x))\n",
    "    print(\"Group Count: \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group vector: [2 2 4 4 1 4 0 0 4 0 4 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Unique groups: [0 1 2 3 4]\n",
      "There are 5 different groups\n"
     ]
    }
   ],
   "source": [
    "# This prints the clusters found by applying KMeans to the PCA of the data with\n",
    "# the n_clusters argument\n",
    "n_clusters = 5\n",
    "labels = KMeans(n_clusters=n_clusters, random_state=4).fit(pca).labels_\n",
    "print(\"Group vector: \"+str(labels))\n",
    "print(\"Unique groups: \"+str(np.unique(labels)))\n",
    "print(\"There are \"+str(len(np.unique(labels)))+\" different groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_choices_projections\n",
      "random_choices_vectors\n",
      "random_choices_names\n",
      "random_choices_id\n",
      "random_choices_speedups\n",
      "first_choices_projections\n",
      "center_choices_projections\n",
      "center_choices_vectors\n",
      "center_choices_names\n",
      "center_choices_id\n",
      "center_choices_speedups\n",
      "center_choices_errors\n",
      "complete_groups\n",
      "first_choices_vectors\n",
      "first_choices_names\n",
      "first_choices_id\n",
      "group_count\n",
      "total_runtime\n",
      "group_number\n",
      "errors\n",
      "speedups\n",
      "number_of_kernels\n",
      "random_per_K\n"
     ]
    }
   ],
   "source": [
    "# Keys for the result object\n",
    "for key in results.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6029342539891235,\n",
       " 0.2515317369681213,\n",
       " 0.2480030403286521,\n",
       " 0.20360076841346875,\n",
       " 0.1417213902736887,\n",
       " 0.06342859241343914,\n",
       " 0.01290918881733483,\n",
       " 0.19104994533078895,\n",
       " 0.00047700711594153686,\n",
       " 0.09569361682959641,\n",
       " 0.09694929209562891,\n",
       " 0.1121320005702361,\n",
       " 0.046034898902943586,\n",
       " 0.023535689554819796,\n",
       " 0.0757274831887754,\n",
       " 0.04074227748500013,\n",
       " 0.029817404386681706,\n",
       " 0.0024332196266228564,\n",
       " 0.008450794197165594]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Errors as a function of clusters\n",
    "# Note that different runs will results in different errors as\n",
    "# KMeans is non-deterministic, and the composition of groups will not \n",
    "# be always the same.\n",
    "\n",
    "results['errors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
