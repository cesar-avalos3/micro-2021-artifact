% LaTeX template for Artifact Evaluation V20201122
%
% Prepared by 
% * Grigori Fursin (cTuning foundation, France) 2014-2020
% * Bruce Childers (University of Pittsburgh, USA) 2014
%
% See examples of this Artifact Appendix in
%  * SC'17 paper: https://dl.acm.org/citation.cfm?id=3126948
%  * CGO'17 paper: https://www.cl.cam.ac.uk/~sa614/papers/Software-Prefetching-CGO2017.pdf
%  * ACM ReQuEST-ASPLOS'18 paper: https://dl.acm.org/citation.cfm?doid=3229762.3229763
%
% (C)opyright 2014-2020
%
% CC BY 4.0 license
%

\documentclass{sigplanconf}

\usepackage{hyperref}
\usepackage{listings}

\begin{document}

\special{papersize=8.5in,11in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When adding this appendix to your paper, 
% please remove above part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\section{Artifact Appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

This artifact provides the entire environment to perform Principal Kernel Analysis (PKA). 
The simulation framework, benchmarks, and tools necessary to run PKA are all either included
or generated by the Dockerfile. Scripts are offered to validate the results presented in the paper.

\subsection{Artifact check-list (meta-information)}

%{\em Obligatory. Use just a few informal keywords in all fields applicable to your artifacts
%and remove the rest. This information is needed to find appropriate reviewers and gradually 
%unify artifact meta information in Digital Libraries.}

{\small
\begin{itemize}
  \item {\bf Algorithm: Principal kernel analysis}
  \item {\bf Program: Python, CUDA}
  \item {\bf Compilation: GCC 7}
  %\item {\bf Transformations: }
  %\item {\bf Binary: }
  %\item {\bf Model: }
  \item {\bf Data set: Rodinia, Parboil, Polybench, MLPerf }
  \item {\bf Run-time environment: Dockerfile }
  \item {\bf Hardware: Nvidia V100, Nvidia RTX 2060, Nvidia RTX 3070}
  % \item {\bf Run-time state: }
  % \item {\bf Execution: }
  \item {\bf Metrics: cycles}
  \item {\bf Output: Table }
  % \item {\bf Experiments: }
  \item {\bf How much disk space required (approximately)?: 50 GB regular (+8 TB complete)}
  \item {\bf How much time is needed to prepare workflow (approximately)?: 5 hours }
  \item {\bf How much time is needed to complete experiments (approximately)?: 3 hours }
  \item {\bf Publicly available?: Yes}
 % \item {\bf Code licenses (if publicly available)?: }
 % \item {\bf Data licenses (if publicly available)?: }
  \item {\bf Workflow framework used?: GPGPU-Sim, Accel-Sim}
 % \item {\bf Archived (provide DOI)?: }
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}
We provide a Dockerfile to compose the experimental environment. Within the Dockerfile we offer options
via commented blocks to download additional benchmarks and their associated traces. Since running all applications 
is impossible, we by default only enable all of those which can be simulated in under 3 hours. We note the total size 
of the traces and the visualizer logs for the entire suite can weigh in the order of 10 TB.

\subsubsection{How to access}

The scripts and the Dockerfile are included in the Zenodo link. The Dockerfile will
generate an environment with the CUDA toolkit 11.2. It will pull the Accel-Sim-Framework and
gpu-app-collection repositories from github, set everything up and compile.

\subsubsection{Hardware dependencies}
For the silicon results, the experiment utilized three different GPUs: Nvidia V100, Nvidia RTX 2060 and Nvidia RTX 3070.
For this artifact, we assume only the V100 is present.

For the simulation results a 1000-threaded XEON server was used. 

\subsubsection{Software dependencies}
The host compute should have the CUDA 11.2 toolkit and compatible drivers installed, alongside nvidia-docker2.
The newer versions of the profiler software Nsight Compute and Nsight Systems require special permissions to 
run, 'modprob'ing NVreg\_RestrictProfilingToAdminUsers is necessary.

\subsubsection{Data sets}
The data sets associated with the classic benchmarks will be downloaded. Certain MLPerf applications require special
data sets that cannot be freely distributed, we defer to the documentation by MLPerf on steps to obtaining them.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}

Once the requirements of the CUDA toolkit 11.2, compatible drivers and
nvidia-docker2 have been met, build the Docker environment by running the
included make-docker.sh script.

\begin{lstlisting}[language=bash]
  $ . make_docker.sh
\end{lstlisting}

The dockerfile will pull an Nvidia container with CUDA-11.2 pre-installed and configured. 
The script then installs everything required to run Accel-Sim and the benchmarks, including 
fetching data sets, traces, and compiling the benchmarks. 

Once this initial setup is done, the traces will be downloaded. The total (uncompressed) size of 
the traces for the entire classic benchmarks suite is 5.5 TB, only the smaller ones will be downloaded 
($\approx$ 9 GB compressed, 220 GB uncompressed). The rest of the traces can be obtained by 
uncommenting the option inside the Dockerfile.gpu file, and running make-docker.sh again.
To run the benchmarks we use the run\_docker.sh shell script, which runs the previously generated container.

\begin{lstlisting}[language=bash]
  $ . run_docker.sh
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Experiment workflow}

%On the cycle measurement we used 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and expected results}

Running the shell script file Run\_PKA.sh will generate the big table included in the paper as Table 2,
alongside several pkl files containing the number of principal groups, the principal kernels associated
with each group and their respective weights.

Note that since we assume only the V100 is included, only the columns associated with it will be 
generated. Also the number of applications will be less as some applications require 
months to simulate. (Though the option to simulate everything is included.)

As explained in the paper, some cuDNN applications are expected to generate a different amount of kernels
depending on the level of overhead experienced by the application. We believe that the cuDNN function 
cudnnFindConvolutionForwardAlgorithmEx is somewhat responsible. As a rule of thumb, if the number 
of kernels don't match, we don't consider the workload.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment customization}

Because of the aforementioned issue of simulation run-time, we are by
default only selecting those smaller classic workloads (Rodinia mainly,
and some Polybench applications). 

If the user wants to include more benchmarks, they can uncomment the 
blocks inside the Dockerfile.gpu associated with said benchmark, and 
re-run the make\_docker.sh script. Once the other benchmarks are 
downloaded, the user can modify the Run\_Updateable.sh shell script with  
instructions indicated in said file. The user can also start the 
docker in interactive shell mode and configure things manually.
The command is

\begin{lstlisting}[language=bash]
  $ nvidia-docker run -it micro-2021-pka /bin/bash
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Methodology}

Submission, reviewing and badging methodology:

\begin{itemize}
  \item \url{https://www.acm.org/publications/policies/artifact-review-badging}
  \item \url{http://cTuning.org/ae/submission-20201122.html}
  \item \url{http://cTuning.org/ae/reviewing-20201122.html}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When adding this appendix to your paper, 
% please remove below part
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
