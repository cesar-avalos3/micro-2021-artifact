from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from scipy.spatial import distance
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sys import argv
import gzip
import copy
import json
from pandas import DataFrame
from statsmodels.tsa.arima_model import ARIMA
from ipywidgets import interact_manual, widgets, interactive
from scipy import stats
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from random import randint
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
import random
from scipy import stats
from scipy.spatial import distance
import statistics
import scipy
import paths
import RKS
import RGS
import pickle

def save_variables(proc, filename):
    with open(filename, 'wb') as f:
        pickle.dump(proc, f)
        
def open_variables(filename):
    with open(filename, 'rb') as f:
        proc = pickle.load(f)
    return proc

# RKS --------------------------------------------------------------
# Because the nsight compute program doesn't have an exclusive
# export to CSV option, using the buffer stdout instead, we have to
# get rid of some lines, this function does that. 
def ignore_lines(file):
    # check the first 1000 lines, if the phrase "device__attribute_async_engine_count"
    # is found, else return -99
    # This phrase is always printed by nsight-compute.
    i = 0
    with open(file) as in_file:
        for line in in_file:
            if ("device__attribute_async_engine_count" in line):
                return i
            elif(i > 1000):
                return -99
            i += 1
            
# General open file function, takes in a nsight-compute csv filename, 
# returns a pandas table.
def open_file(filename):
    #print(filename)
    lines_to_ignore = ignore_lines(filename)
    if(lines_to_ignore == -99):
        lines_to_ignore = 0
    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)
    if(table.loc[0,'ID'] != 0.0):
        table = table.drop(0)
        table.index = range(len(table))
    # For certain systems nsight-compute prints
    # comma-deliniated numbers, e.g. 1,000 instead of 1000.
    # We need to get rid of that. 
    table = table.replace(',','', regex=True)
    return table

# List of agnostic features to take into consideration when performing
# the PCA (i.e. only these variables are going to be used)
agnostic_features = ['l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum',
                     'l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum',
                     #'l1tex__t_sectors_pipe_lsu_mem_local_op_ld.sum',
                     'smsp__inst_executed.sum','smsp__thread_inst_executed_per_inst_executed.ratio',
                     #'smsp__sass_inst_executed_op_global_atom.sum',
                     'smsp__inst_executed_op_global_ld.sum',
                     'smsp__inst_executed_op_global_st.sum',
                     'smsp__inst_executed_op_shared_ld.sum',
                     'smsp__inst_executed_op_shared_st.sum',
                     #'smsp__inst_executed_op_surface_atom.sum',
                     #'smsp__inst_executed_op_surface_ld.sum',
                     #'smsp__inst_executed_op_surface_red.sum',
                     #'smsp__inst_executed_op_surface_st.sum',
                     'sass__inst_executed_global_loads',
                     'sass__inst_executed_global_stores',
                     'launch__grid_size']

# Opens the cycles-profiled-only csvs generated by
# the nsight-compute utility, obtained using the 
# run_hw.py under the accel-sim utility.
def open_cycles(filename):
    #print(filename)
    lines_to_ignore = ignore_cycle_lines(filename)
    if(lines_to_ignore == -99):
        lines_to_ignore = 0
    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)
    return table

def ignore_cycle_lines(file):
    # check the first 1000 lines, if the phrase "Section Name" is not found
    # return -99. Same as ignore_lines above, but for the cycles-only version.
    i = 0
    with open(file) as in_file:
        for line in in_file:
            if ("Section Name" in line):
                return i
            elif(i > 1000):
                return -99
            i += 1

def generate_DF(table, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID']):
    x_large = table.loc[:,agnostic_features].values
    x_large = StandardScaler().fit_transform(x_large)
    temp = table.loc[:,agnostic_features]
    if(True):
        for value in agnostic_features:
            if(temp[value].isnull().values.any()):
                print(value)
    for new_col in appendable_columns:
        temp = table[[new_col]]
        if "Name" not in new_col:
            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))
        try:
            x_large = pd.concat([x_large, temp], axis = 1)
        except:
            x_large = pd.concat([pd.DataFrame(x_large), pd.DataFrame(temp)], axis = 1)
    return x_large

def generate_PCA(table, pca_variation=0.9995, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID'], debug=False):
    x_large = table.loc[:,agnostic_features].values
    #x_large = x_large.apply(lambda x: pd.to_numeric(x.astype(str).str.replace(',','')))
    #x_large = x_large.values
    x_large = StandardScaler().fit_transform(x_large)
    temp = table.loc[:,agnostic_features]
    if(True):
        for value in agnostic_features:
            if(temp[value].isnull().values.any()):
                print(value)
    pca_components = pca_variation
    pca = PCA(pca_components)
    principalComponents_Large = pca.fit_transform(x_large)
    principalDf_Large = pd.DataFrame(data = principalComponents_Large, columns = ['principal component '+str(x+1) for x in range(pca.n_components_)])
    for new_col in appendable_columns:
        temp = table[[new_col]]
        if "Name" not in new_col:
            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))
        principalDf_Large = pd.concat([principalDf_Large, temp], axis = 1)
    #finalDf_Large = pd.concat([finalDf_Large,truncated_names], axis = 1)
    #finalDf_Large = pd.concat([finalDf_Large, nsight_csv[['ID']]], axis = 1)
    #finalDf_sorted = finalDf_Large#.sort_values('sm__inst_issued.avg.per_cycle_active [inst/cycle]')
    #%matplotlib inline
    #fig = plt.figure(figsize = (10,10))
    #ax = fig.add_subplot(111) 
    #ax.set_title('PCA components - variance', fontsize = 20)
    #plt.bar(range(pca.n_components_), pca.explained_variance_ratio_, color='black')
    #plt.xlabel('PCA features')
    #plt.ylabel('variance %')
    #plt.xticks(range(pca.n_components_))
    #plt.show()
    # Clustering
    #k_means = KMeans(n_clusters=2, random_state=2).fit(principalComponents_Large)
    #finalDf_sorted['Segments'] = k_means.labels_
    #print(principalDf_Large)
    return principalDf_Large, principalComponents_Large 

# Naively choosing one from each group at random
def kmeans_clustering(dataFrame,principalComponents_dataFrame,starting_from=1, ending_at=20, column_variable='gpc__cycles_elapsed.avg',print_output=False):
    total_runtime = dataFrame[column_variable].sum()
    complete_groups = []
    random_per_K = []
    if(ending_at > len(dataFrame)):
        ending_at = len(dataFrame)+1
    results = {'random_choices_projections':[], 'random_choices_vectors':[], 'random_choices_names':[], 
               'random_choices_id':[], 'random_choices_speedups':[],'first_choices_projections':[],
               'center_choices_projections':[], 'center_choices_vectors':[], 'center_choices_names':[],
               'center_choices_id':[], 'center_choices_speedups':[], 'center_choices_errors': [],
               'complete_groups':[],
               'first_choices_vectors':[], 'first_choices_names':[], 'first_choices_id':[], 'group_count':[], 
               'total_runtime':total_runtime, 'group_number': [], 'errors': [], 'speedups': [], 'number_of_kernels': []}
    for i in range(starting_from, ending_at):
        random_choices = [] # Randomly select a kernel from the group
        random_choices_name = []
        random_choices_id = []
        mean_choices = []   # Select the kernel with the value closest to the mean
        max_choices = []    # Select the largest kernel value
        first_choices = []   # Selects the first chronological value
        first_choices_name = []
        first_choices_id = []
        center_choices_id = []
        closest_to_mean = []
        closest_to_mean_names = []
        group_count = []    # Number of elements inside the cluster i
        complete_groups_df = []
        k_means = KMeans(n_clusters=i, random_state=4).fit(principalComponents_dataFrame)
        dataFrame['Segments'] = k_means.labels_
        center_ids = cluster_centers(dataFrame,principalComponents_dataFrame,k_means)
        per_group_random = []
        for group in np.unique(k_means.labels_):
            temp_df = dataFrame.loc[dataFrame['Segments'] == group]
            #closest_to_mean.append(temp_df.loc[center_ids[group], column_variable])
            #closest_to_mean_names.append(temp_df.loc[center_ids[group], 'Kernel Name'])
            complete_groups_df.append(temp_df)
            temp_df.index = range(len(temp_df))
            #value_first = temp_df.loc[0,column_variable]
            first_choices.append(temp_df.loc[0,column_variable])
            first_choices_name.append(temp_df.loc[0,'Kernel Name'])
            first_choices_id.append(temp_df.loc[0,'ID'])
            #center_choices_id.append(center_ids[group])
            temp_df_sorted = temp_df.sort_values(column_variable)
            temp_df_sorted.index = range(len(temp_df_sorted))
            group_count.append(len(temp_df))
            random_vals = []
            random_names = []
            random_ids = []
            random_choice = random.randint(0,len(temp_df)-1)
            random_choice_name = ''
            for i in range(10):
                random_choice_ = random.randint(0,len(temp_df))
                random_choice_name = ''
                try:
                    random_vals.append(temp_df_sorted.loc[random_choice_,column_variable] * len(temp_df))
                    random_names.append(temp_df_sorted.loc[random_choice, 'Kernel Name'])
                    random_ids.append(random_choice_)
                except:
                    random_vals.append(temp_df_sorted.loc[0,column_variable] * len(temp_df))
                    random_names.append(temp_df_sorted.loc[0, 'Kernel Name'])
                    random_ids.append(random_choice)
            max_choice = temp_df_sorted.loc[len(temp_df)-1, column_variable]
            try:
                value_mean = temp_df_sorted.loc[int(len(temp_df)/2),column_variable]
            except:
                print(int(len(temp_df)/2))
            try:
                value_random = temp_df_sorted.loc[random_choice,column_variable]
                random_choice_name = temp_df_sorted.loc[random_choice, 'Kernel Name']
            except:
                print("Why would this happen?")
                print("Random Choice: "+str(random_choice))
                print("Length dataFrame: "+str(len(temp_df_sorted)))
                value_random = temp_df_sorted.loc[0,column_variable]
                random_choice_name = temp_df_sorted.loc[0,'Kernel Name']
            #value_random = 0
            random_choices.append(value_random)
            random_choices_id.append(random_choice)
            random_choices_name.append(random_choice_name)
            mean_choices.append(value_mean)
            max_choices.append(max_choice)
            per_group_random.append({'random_vals': random_vals, 'random_names': random_names, 'random_ids': random_ids})
        complete_groups.append(complete_groups_df)
        random_runtime = [random_choices[i] * group_count[i] for i in range(len(random_choices))]
        mean_runtime = [mean_choices[i] * group_count[i] for i in range(len(random_choices))]
        max_runtime = [max_choices[i] * group_count[i] for i in range(len(random_choices))]
        first_runtime = [first_choices[i] * group_count[i] for i in range(len(random_choices))]
        #closest_runtime = [closest_to_mean[i] * group_count[i] for i in range(len(random_choices))]
        random_per_K.append(per_group_random)
        if(print_output):
            print('For '+str(i)+ ' groups')
            print('----------------------------------------------')
            print(dataFrame[column_variable])
            print('The actual run time is: '+str(total_runtime))
            print('----------------- Projections ----------------')
            print(' ')
            print('The random value run time is: '+str(np.sum(random_runtime)))
            print('The error is '+ str(np.sum(random_runtime) / total_runtime ))
            print('The mean value run time is: '+str(np.sum(mean_runtime)))
            print('The error is '+ str(np.sum(mean_runtime) / total_runtime ))
            print('The max value run time is: '+str(np.sum(max_runtime)))
            print('The error is '+ str(np.sum(max_runtime) / total_runtime ))
            print('The first value run time is: '+str(np.sum(first_runtime)))
            print('The error is '+ str(np.sum(first_runtime) / total_runtime ))
            print('The speedup is '+ str(total_runtime / np.sum(first_choices)))
            print('The reduced number of kernels '+str(len(first_runtime)))
            print('The total number of kernels '+str(len(dataFrame)))
            print('The first choice vector is '+str(first_choices))
            print('The number of elements per group is '+str(group_count))
            print('The names of the first choices are '+str(first_choices_name))
            print('The kernel IDs of the first choices are '+str(first_choices_id))
        #print('Their product is '+str( [first_choices[i] * group_count[i] for i in range(len(first_choices))] ))
            print(' ')
        results['errors'].append(np.abs((np.sum(first_runtime)-total_runtime)) / total_runtime)
        results['speedups'].append(total_runtime / np.sum(first_choices))
        results['first_choices_vectors'].append(first_choices)
        results['first_choices_projections'].append(first_runtime)
        results['first_choices_names'].append(first_choices_name)
        results['first_choices_id'].append(first_choices_id)
        results['random_choices_vectors'].append(random_choices)
        results['random_choices_projections'].append(random_runtime)
        results['random_choices_names'].append(random_choices_name)
        results['random_choices_id'].append(random_choices_id)
        #results['center_choices_projections'].append(closest_runtime)
        #results['center_choices_id'].append(center_choices_id)
        #results['center_choices_vectors'].append(closest_to_mean)
        #results['center_choices_names'].append(closest_to_mean_names)
        #results['center_choices_errors'].append(np.abs((total_runtime - np.sum(closest_runtime))) / total_runtime)
        results['group_count'].append(group_count)
        results['group_number'].append(group)
        results['complete_groups'].append(complete_groups)
        results['number_of_kernels'].append(len(dataFrame))
        results['random_per_K'] = random_per_K
        #print(finalDf_sorted.loc[finalDf_sorted['Segments'] == group])
    return results
#kmeans_clustering(finalDf_sorted, principalComponents_Large)

# Returns the ID of the element closest to the group's centers
def cluster_centers(dataFrame, principalComponents_dataFrame, k_means, debug=False):
    centers_vector = []
    centers_ids = []
    clusters_centers = k_means.cluster_centers_
    temp_pca_df = dataFrame.copy()
    #labels = ['principal component '+str(x+1) for x in range(len(np.unique(k_means.labels_)))]
    for group in np.unique(k_means.labels_):
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_grouped_pca_df = temp_df.filter(regex='principal')
        original_indeces = temp_grouped_pca_df.index.values.tolist()
        temp_grouped_pca_df.index = range(len(temp_grouped_pca_df))
        minimum_distance = temp_grouped_pca_df.loc[0,:].values.tolist()
        minimum_index = original_indeces[0]
        #print(minimum_distance)
        if(debug):
            print('The clusters centers are: '+str(clusters_centers[group]))
            print('The first data point is : '+str(minimum_distance))
        minimum_distance = distance.euclidean(minimum_distance, clusters_centers[group])
        if(debug):
            print(minimum_distance)
        for i in range(len(temp_grouped_pca_df)):
            data_point = temp_grouped_pca_df.loc[i,:].values.tolist()
            temp_distance = distance.euclidean(data_point, clusters_centers[group])
            if(temp_distance < minimum_distance):
                minimum_distance = temp_distance
                minimum_index = original_indeces[i]
        if(debug):
            print(minimum_distance, minimum_index)
        centers_ids.append(minimum_index)
    if(debug):
        print('Returns from function \nNew iteration\n\n')
    return centers_ids

def point_closest_to_all(dataFrame, principalComponents_dataFrame, k_means, debug=False):
    closest_ids = []
    for group in np.unique(k_means.labels_):
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_grouped_pca_df = temp_df.filter(regex='principal')
        original_indeces = temp_grouped_pca_df.index.value.tolist()
    
def check_results(dataFrame, best_choices, group_counts, column_variable='gpc__cycles_elapsed.avg',print_debug=True):
    result_dictionary = {}
    total_runtime = pd.to_numeric(dataFrame[column_variable]).sum()
    runtime_list = []
    projected_runtime = 0.0
    for i in range(len(best_choices)):
        index = int(best_choices[i])
        runtime_list.append(float(dataFrame.loc[index, column_variable]))
        projected_runtime += float(dataFrame.loc[index, column_variable]) * group_counts[i]
    if(print_debug):
        print('The actual runtime is: '+str(total_runtime))
        print('The projected runtime with the borrowed K-Means is: '+str(projected_runtime))
        print('The ratio is '+str(projected_runtime/total_runtime))
    result_dictionary = {'total_runtime': total_runtime, 'projected_runtime': projected_runtime,
                         'error': np.abs(projected_runtime - total_runtime)/total_runtime,
                         'speedup': total_runtime / np.sum(runtime_list), 'reduced_runtime': np.sum(runtime_list), 
                         'runtime_list': runtime_list}
    return result_dictionary 

def open_all(profile_paths):
    Beeg_Table = {}
    list_of_apps = []
    for app in profile_paths:
        try:
            print(app)
            table = open_file(profile_paths[app])
            #DF_Large, PC_Large = generate_PCA(table)
            #results = kmeans_clustering(DF_Large, PC_Large)
            Beeg_Table[app] = table
            list_of_apps.append(app)
        except Exception as e:
            print(app)
            print(e)
            print("Has a problem")
    return Beeg_Table, list_of_apps

def process_all(Beeg_Table, list_of_apps, extra_archs=[''], min_error_arg = 0.05, all_paths_ampere = [], all_paths_turing = []):
    list_of_speedups = []
    list_of_errors = []
    best_choices_dict = {}
    dict_best_choices = {}
    for app in list_of_apps:
        min_error = 99
        index_min_error = 99
        min_error_speedup = 99
        group_counts_min = []
        kernel_ids_min = []
        first_error_less_than_4_percent = []
        first_error_less_than_3_percent = []
        first_error_less_than_2_percent = []
        for i in range(len(Beeg_Table[app]['errors'])):
            error = Beeg_Table[app]['errors'][i]
            #error = Beeg_Table[app]['center_choices_errors'][i]
            speedup = Beeg_Table[app]['speedups'][i]
            group_counts = Beeg_Table[app]['group_count'][i]
            #kernel_ids = Beeg_Table[app]['center_choices_id'][i]
            kernel_ids = Beeg_Table[app]['first_choices_id'][i]
            print("The error is: "+str(error))
            print("The speedup is: "+str(speedup))
            if(error < min_error):
                min_error = error
                index_min_error = i
                min_error_speedup = speedup
                group_counts_min = group_counts
                kernel_ids_min = kernel_ids
            if(error < min_error_arg):
                first_error_less_than_4_percent.append([error,i,speedup, group_counts, kernel_ids])
                if(error < min_error_arg/2.0):
                    first_error_less_than_3_percent.append([error,i,speedup])
                    if(error < min_error_arg/4.0):
                        first_error_less_than_2_percent.append([error,i,speedup])
        if(len(first_error_less_than_4_percent)==0):
            first_error_less_than_4_percent.append([min_error, index_min_error, min_error_speedup,group_counts_min, kernel_ids_min])
        list_of_speedups.append(first_error_less_than_4_percent[0][2])
        list_of_errors.append(first_error_less_than_4_percent[0][0])
        best_choices_dict[app] = [first_error_less_than_4_percent[0][4],
                                  first_error_less_than_4_percent[0][3],
                                  first_error_less_than_4_percent[0][2]]
        dict_best_choices[app] = {'kernel_ids':first_error_less_than_4_percent[0][4], 
                                  'group_counts': first_error_less_than_4_percent[0][3], 
                                  'speedup': first_error_less_than_4_percent[0][2], 
                                  'error': first_error_less_than_4_percent[0][0]}

        #print(best_choices_dict[app])

    extra_archs_results = {}
    turing_results = {}
    if('turing' in extra_archs):
        list_of_turing_apps = []
        list_of_turing_results = []
        list_of_turing_speedups = []
        list_of_turing_names = []
        list_of_turing_total_runtimes = []
        for app in all_paths_turing:
            #print(table)
            try:
                table = open_file(all_paths_turing[app])
                time = pd.to_numeric(table['gpc__cycles_elapsed.avg']).sum()
                DF_Large = generate_DF(table)
                list_of_turing_apps.append(app)
                temp_results = check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1])
                turing_results[app] = temp_results
                list_of_turing_results.append(temp_results)
                list_of_turing_speedups.append(np.sum(best_choices_dict[app][0])/temp_results[0])
                list_of_turing_names.append(app)
                list_of_turing_total_runtimes.append(time)
            except Exception as e:
                print(e)
                pass
        extra_archs_results['turing'] = turing_results
#        extra_archs_results['turing'] = {'results': list_of_turing_results, 'speedups': list_of_turing_speedups, 'names': list_of_turing_names, 'total_time':list_of_turing_total_runtimes}
            #print(check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1]))
            
    if('ampere' in extra_archs):
        list_of_ampere_apps = []
        list_of_ampere_results = []
        list_of_ampere_speedups = []
        list_of_ampere_names = []
        for app in all_paths_ampere:
            try:
                table = open_file(all_paths_ampere[app])
                DF_Large = generate_DF(table)
                list_of_ampere_apps.append(app)
                temp_results = check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1])
                list_of_ampere_results.append(temp_results)
                list_of_ampere_speedups.append(temp_results[0]/np.sum(best_choices_dict[app][0]))
                list_of_ampere_names.append(app)
            except:
                pass
        extra_archs_results['ampere'] = {'results': list_of_ampere_results, 'speedups': list_of_ampere_speedups, 'names': list_of_ampere_names}

    return best_choices_dict, list_of_errors, list_of_speedups, dict_best_choices, extra_archs_results

def kernel_distribution_per_kmean_groups(dataFrame,principalComponents_dataFrame,best_choice=5, kernel_name = 'Kernel Name'):
    #for i in range(starting_from, ending_at):
    histogram_cu = {}
    histogram_fn = {}
    total_runtimes = []
    k_means = KMeans(n_clusters=best_choice, random_state=4).fit(principalComponents_dataFrame)
    dataFrame['Segments'] = k_means.labels_
    print(np.unique(k_means.labels_))
    unique_cufunction_names = dataFrame[kernel_name].unique()
    unique_kernel_names = dataFrame[kernel_name].unique()
    for group in np.unique(k_means.labels_):
        for key in unique_cufunction_names:
            histogram_cu[key] = 0
        for key in unique_kernel_names:
            histogram_fn[key] = 0
        # Extract only the kernels related to the group inside the loop
        fig = plt.figure(figsize = (20,10))
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_df.index = range(len(temp_df))
        runtime = temp_df['gpc__cycles_elapsed.avg'].sum()
        total_count = len(temp_df)
        for i in range(len(temp_df)):
            cu_name = temp_df.loc[i, kernel_name]
            func_name = temp_df.loc[i, kernel_name]
            histogram_cu[cu_name] += 1
            histogram_fn[func_name] += 1
        total_runtimes.append(runtime)
        ax = fig.add_subplot(121)
        ax.set_title('K-Means Group '+str(group),fontsize=30)
        chart = plt.bar(histogram_cu.keys(), histogram_cu.values(), 1)
        ax.legend(['Kernel count: '+str(total_count)+'\nTotal Runtime: '+f"{runtime:,}"], fontsize=20)
        ax.set_xticklabels(histogram_cu.keys(),rotation=90)
        ax = fig.add_subplot(122)
        chart = plt.bar(histogram_fn.keys(), histogram_fn.values(), 1)
        ax.set_xticklabels(histogram_fn.keys(),rotation=90)
    fig = plt.figure(figsize = (20,10))
    ax = fig.add_subplot(111)
    ax.set_title("Total Runtimes as a function of Group Id", fontsize=30)
    plt.bar(range(len(total_runtimes)), total_runtimes)
    plt.xlabel('Group ID',fontsize=20)
    plt.ylabel('Run time',fontsize=20)
        #temp_df['CUfunction'].hist()

#PKS_Turing_dict = open_variables('PKS_Turing')
#PKS_Ampere_dict = open_variables('PKS_Ampere')
#PKS_Volta_dict  = open_variables('PKS_Volta' )


# We open the viz log files, parse the log,
# find the different kernels by finding the points at which
# the cycle count goes to 0, separate them by kernel
# Adapted from Roland Green's code

# Returns a list of logfile paths
def get_paths(directory):
    log_paths = ""
    output_paths = ""
    for root, dirs, files in os.walk(directory):
        for f in files:
            #print(f)
            if f.endswith(".log.gz"):
                log_paths = os.path.join(root, f)
            if ".0.o" in f:
                output_paths = os.path.join(root, f)
    #print(log_paths + " " + output_paths)
    return log_paths, output_paths

# Takes in the viz and output path
# Generates a list of kernel dictionaries containing the
# data points' cycle, instructions count, and the number of completed ctas since
# last data point.
# ----------------  original parse lines ------------------------
#    Certain things are assumed here, "CTA/core = " only gets 
#    printed if the kernel struct changed from the previous kernel
#    gpgpusim/shader.cc : 3228 @ commit 3f051d4 - gpgpusim official repo
#    prev_ctas = 0
#    for line in lines:
#        if "CTA/core = " in line:
#            wave_sizes.append(int(line.split()[4][:-1]) * 80)
#            continue
#        if "gpu_tot_issued_cta" in line:
#            issued_ctas = int(line.split()[-1])
#            total_ctas.append(issued_ctas - prev_ctas)
#            prev_ctas = issued_ctas
#        if "kernel_name = " in line:
#            kernel_names.append(line.split()[-1])
def parse_lines(log_path, output_path):
    # Dump the lines from the file
    with gzip.open(log_path, 'rt') as f:
    # Get the cycles, instructions, and CTAs counts
        cycles = []
        instructions = []
        ctas = []
        for line in f:
            if "globalcyclecount" in line:
                cycles.append(int(line.split(':')[1]))
                continue
            if "globalinsncount" in line:
                instructions.append(int(line.split(':')[1]))
                continue
            if "ctas_completed" in line:
                ctas.append(int(line.split(':')[1]))
                continue
    cycles = np.array(cycles)
    instructions = np.array(instructions)
    ctas = np.array(ctas)
    kernel_indeces = np.where(cycles == cycles[0])
    kernel_indeces = kernel_indeces[0]
    number_of_kernels = len(kernel_indeces)
    kernel_list = []
    for i in range(len(kernel_indeces)-1):
        temp_cycles = cycles[kernel_indeces[i]:kernel_indeces[i+1]]
        #print(temp_cycles)
        temp_ctas = ctas[kernel_indeces[i]:kernel_indeces[i+1]]
        temp_instructions = instructions[kernel_indeces[i]:kernel_indeces[i+1]]
        temp_ipcs = temp_instructions / temp_cycles
        kernel_list.append({'cycles': temp_cycles, 'instructions':temp_instructions, 'ctas':temp_ctas, 'ipcs': temp_ipcs})

    temp_cycles = cycles[kernel_indeces[-1]:]
    temp_ctas = ctas[kernel_indeces[-1]:]
    temp_instructions = instructions[kernel_indeces[-1]:]
    temp_ipcs = temp_instructions / temp_cycles
    kernel_list.append({'cycles': temp_cycles, 'instructions':temp_instructions, 'ctas':temp_ctas, 'ipcs':temp_ipcs})
    
    for kernel in kernel_list:
        if(len(kernel['ctas'])>0): 
            kernel['ctas'][0] = 0
    
    with open(output_path, 'r') as f:
        lines = f.readlines()

    # Get the size of each wave, and total number of CTAs
    wave_sizes = []
    total_ctas = []
    kernel_names = []

    kernel_names, wave_sizes, total_ctas = parse_kernel_info_format_2(output_path)
    
    kernel_info = {
            "kernel_names" : kernel_names,
            "wave_sizes" : wave_sizes,
            "total_ctas" : total_ctas,
            }
    return kernel_list, kernel_info

def parse_lines_dictionary(log_path, output_path, format_type = 3):
    with open(output_path, 'r') as f:
        lines = f.readlines()

    # Get the size of each wave, and total number of CTAs
    wave_sizes = []
    total_ctas = []
    kernel_names = []
    kernel_ids = []
    kernel_info = {}
    if(format_type == 3):
        kernel_info = parse_kernel_info_format_3(output_path)
    elif(formate_type == 4):
        pass
    else:
        kernel_info = parse_kernel_info_format_2(output_path)

    #print(sorted([ int(x) for x in list(kernel_info.keys())] ))
    #print("Sorted key above")
    kernel_ids = sorted([ int(x) for x in list(kernel_info.keys())] )
    
    # Dump the lines from the file
    with gzip.open(log_path, 'rt') as f:
        lines = f.readlines()

    # Get the cycles, instructions, and CTAs counts
    first_kernel_id = kernel_ids[0]
    cycles = []
    instructions = []
    ctas = []
    gpu_n_mem_read_global = []
    gpu_n_mem_write_global = []
    gpu_n_mem_req_global = []
    Ltwowritemiss = []
    Ltwowritehit = []
    Ltworeadmiss = []
    Ltworeadhit = []
    dramutil = []
    tmp_dram_util = []
    dramutil_vector = []
    n_command_vector = []
    tmp_n_command_vector = []
    for line in lines:
        if "globalcyclecount" in line:
            cycles.append(int(line.split(':')[1]))
        elif "globalinsncount" in line:
            instructions.append(int(line.split(':')[1]))
        elif "ctas_completed" in line:
            ctas.append(int(line.split(':')[1]))
        elif "gpgpu_n_mem_read_global" in line:
            gpu_n_mem_read_global.append(int(line.split(':')[1]))
        elif "gpgpu_n_mem_write_global" in line:
            gpu_n_mem_write_global.append(int(line.split(':')[1]))
        elif "Ltwowritemiss" in line:
            Ltwowritemiss.append(int(line.split(':')[1]))
        elif "Ltwowritehit" in line:
            Ltwowritehit.append(int(line.split(':')[1]))
        elif "Ltworeadmiss" in line:
            Ltworeadmiss.append(int(line.split(':')[1]))
        elif "Ltworeadhit" in line:
            Ltworeadhit.append(int(line.split(':')[1]))
        elif "dramncmd" in line:
            if(': 31 ' in line):
                tmp = line.split(':')
                tmp = tmp[1]
                tmp = tmp.split(' ')
                tmp = tmp[2]
                tmp = float(tmp)
                tmp_n_command_vector.append(tmp)
                n_command_vector.append(tmp_n_command_vector)
                tmp_n_command_vector = []
            else:
                tmp = line.split(':')
                tmp = tmp[1]
                tmp = tmp.split(' ')
                tmp = tmp[2]
                tmp = float(tmp)
                tmp_n_command_vector.append(tmp)
        elif 'dramutil' in line:
            if(': 31 ' in line):
                tmp = line.split(':')
                tmp = tmp[1]
                tmp = tmp.split(' ')
                tmp = tmp[2]
                tmp = float(tmp)
                tmp_dram_util.append(tmp)
                dramutil.append(np.sum(tmp_dram_util))
                dramutil_vector.append(tmp_dram_util)
                tmp_dram_util = []
            else:
                tmp = line.split(':')
                tmp = tmp[1]
                tmp = tmp.split(' ')
                tmp = tmp[2]
                tmp = float(tmp)
                tmp_dram_util.append(tmp)
    cycles = np.array(cycles)
    instructions = np.array(instructions)
    ctas = np.array(ctas)
    kernel_indeces = np.where(cycles == cycles[0])
    kernel_indeces = kernel_indeces[0]
    number_of_kernels = len(kernel_indeces)
    kernel_list = {}
    for i in range(len(kernel_indeces)-1):
        temp_cycles = cycles[kernel_indeces[i]:kernel_indeces[i+1]]
        #print(temp_cycles)
        temp_ctas = ctas[kernel_indeces[i]:kernel_indeces[i+1]]
        temp_instructions = instructions[kernel_indeces[i]:kernel_indeces[i+1]]
        temp_ipcs = temp_instructions / temp_cycles
        temp_memory_reads  =  gpu_n_mem_read_global[kernel_indeces[i]:kernel_indeces[i+1]]
        temp_memory_writes = gpu_n_mem_write_global[kernel_indeces[i]:kernel_indeces[i+1]]
        try:
            temp_Ltwowritemiss  =  Ltwowritemiss[kernel_indeces[i]:kernel_indeces[i+1]]
            temp_Ltwowritehit = Ltwowritehit[kernel_indeces[i]:kernel_indeces[i+1]]
            temp_Ltworeadmiss  =  Ltworeadmiss[kernel_indeces[i]:kernel_indeces[i+1]]
            temp_Ltworeadhit = Ltworeadhit[kernel_indeces[i]:kernel_indeces[i+1]]
            temp_dramutil = dramutil[kernel_indeces[i]:kernel_indeces[i+1]]
            temp_ncommands = n_command_vector[kernel_indeces[i]:kernel_indeces[i+1]]
            temp_dramutil_vector = dramutil_vector[kernel_indeces[i]:kernel_indeces[i+1]]
            
        except:
            temp_Ltwowritemiss = []
            temp_Ltwowritehit = []
            temp_Ltworeadmiss = []
            temp_Ltworeadhit = []
            temp_dramutil = []
            temp_ncommands = []
            temp_dramutil_vector = []
        kernel_list[str(kernel_ids[i])] = {'cycles': temp_cycles, 'instructions':temp_instructions, 
                                           'ctas':temp_ctas, 'ipcs': temp_ipcs, 'mem_loads': temp_memory_reads, 
                                           'mem_writes': temp_memory_writes, 'Ltwowritemiss': temp_Ltwowritemiss,
                                           'Ltwowritehit': temp_Ltwowritehit, 'Ltworeadmiss':temp_Ltworeadmiss, 
                                           'Ltworeadhit': temp_Ltworeadhit, 'dramutil': temp_dramutil,
                                           'ncommands': temp_ncommands, 'dramutil_vector':temp_dramutil_vector }
        
    temp_cycles = cycles[kernel_indeces[-1]:]
    temp_ctas = ctas[kernel_indeces[-1]:]
    temp_instructions = instructions[kernel_indeces[-1]:]
    temp_ipcs = temp_instructions / temp_cycles
    temp_memory_reads  =  gpu_n_mem_read_global[kernel_indeces[-1]:]
    temp_memory_writes = gpu_n_mem_write_global[kernel_indeces[-1]:]

    try:
        temp_Ltwowritemiss  =  Ltwowritemiss[kernel_indeces[-1]:]
        temp_Ltwowritehit = Ltwowritehit[kernel_indeces[-1]:]
        temp_Ltworeadmiss  =  Ltworeadmiss[kernel_indeces[-1]:]
        temp_Ltworeadhit = Ltworeadhit[kernel_indeces[-1]:]
        temp_dramutil = dramutil[kernel_indeces[-1]:]
        temp_ncommands = n_command_vector[kernel_indeces[-1]:]
        temp_dramutil_vector = dramutil_vector[kernel_indeces[-1]:]
    except:
        temp_Ltwowritemiss = []
        temp_Ltwowritehit = []
        temp_Ltworeadmiss = []
        temp_Ltworeadhit = []
        temp_dramutil = []
        temp_ncommands = []
        temp_dramutil_vector = []
    kernel_list[str(kernel_ids[-1])] = {'cycles': temp_cycles, 'instructions':temp_instructions, 
                                           'ctas':temp_ctas, 'ipcs': temp_ipcs, 'mem_loads': temp_memory_reads, 
                                           'mem_writes': temp_memory_writes, 'Ltwowritemiss': temp_Ltwowritemiss,
                                           'Ltwowritehit': temp_Ltwowritehit, 'Ltworeadmiss':temp_Ltworeadmiss, 
                                           'Ltworeadhit': temp_Ltworeadhit, 'dramutil': temp_dramutil,
                                           'ncommands': temp_ncommands, 'dramutil_vector':temp_dramutil_vector }
    
    for kernel in kernel_list:
        if(len(kernel_list[kernel]['ctas'])>0): 
            kernel_list[kernel]['ctas'][0] = 0
    return kernel_list, kernel_info

def parse_only_output_file(output_path, output_format = 3):
    wave_sizes = []
    total_ctas = []
    kernel_names = []
    if(output_format == 3):
        kernel_names, wave_sizes, total_ctas = parse_kernel_info_format_3(output_path)
    else:
        kernel_names, wave_sizes, total_ctas = parse_kernel_info_format_2(output_path)
        
    kernel_info = {
            "kernel_names" : kernel_names,
            "wave_sizes" : wave_sizes,
            "total_ctas" : total_ctas,
            }
    return kernel_info

def parse_kernel_info_format_1(output_path):
    with open(output_path, 'r') as f:
        lines = f.readlines()
    wave_sizes = []
    total_ctas = []
    kernel_names = []
    prev_ctas = 0
    current_kernel = 1
    last_cta_size = 0

    for i in range(len(lines)): 
        if "pushing kernel " in lines[i]:
            i += 2
            if "CTA/core = " in lines[i]:
                wave_temp = int(lines[i].split()[4][:-1])
                last_cta_size = wave_temp
                #wave_sizes.append(int(line.split()[4][:-1]) * 80)
            else:
                wave_temp = last_cta_size
            shaders = 1
            i += 2
            while("bind to kernel" in lines[i]):
                shaders += 1
                i += 1
            wave_sizes.append(wave_temp * shaders)
        if "gpu_tot_issued_cta" in lines[i]:
            issued_ctas = int(lines[i].split()[-1])
            total_ctas.append(issued_ctas - prev_ctas)
            prev_ctas = issued_ctas
            i+=1
        if "kernel_name = " in lines[i]:
            kernel_names.append(lines[i].split()[-1])
            i += 1        
    return kernel_names, wave_sizes, total_ctas

def parse_kernel_info_format_2(output_path):
    with open(output_path, 'r') as f:
        lines = f.readlines()
        
    wave_sizes = []
    total_ctas = []
    kernel_names = []
    prev_ctas = 0
    current_kernel = 1
    last_cta_size = 0
    kernel_ids = []
    kernel_total_cycles = []
    
    for i in range(len(lines)): 
        if "Reconfigure L1 " in lines[i]:
            #print("Found the line")
            #print(lines[i-1])
            i -= 1
            if "CTA/core = " in lines[i]:
                wave_temp = int(lines[i].split()[4][:-1])
                last_cta_size = wave_temp
                #wave_sizes.append(int(line.split()[4][:-1]) * 80)
            else:
                wave_temp = last_cta_size
            #shaders = 1
            #i += 3
            #while("bind to kernel" in lines[i]):
            #    shaders += 1
            #    i += 2
            wave_sizes.append(wave_temp * 80)
        if "gpu_tot_issued_cta" in lines[i]:
            issued_ctas = int(lines[i].split()[-1])
            total_ctas.append(issued_ctas - prev_ctas)
            prev_ctas = issued_ctas
            i+=1
        if "kernel_name = " in lines[i]:
            kernel_names.append(lines[i].split()[-1])
            i += 1
        if "kernel id = " in lines[i]:
            kernel_ids.append(lines[i].split()[-1])
            i += 1
        if "gpu_sim_cycle" in lines[i]:
            i += 1
            kernel_total_cycles.append(lines[i].split()[-1])
    kernel_info_dict = {}
    for kernel in range(len(kernel_names)):
        current_kernel = kernel_ids[kernel]
        kernel_info_dict[current_kernel] = {'kernel_names': kernel_names[kernel],
                                            'wave_sizes': wave_sizes[kernel],
                                            'total_ctas': total_ctas[kernel],
                                            'kernel_ids': kernel_ids[kernel],
                                            'total_cycles': kernel_total_cycles[kernel]}
    return kernel_info_dict

def parse_kernel_info_format_3(output_path):
    with open(output_path, 'r') as f:
        lines = f.readlines()
        
    wave_sizes = []
    total_ctas = []
    kernel_names = []
    prev_ctas = 0
    current_kernel = 1
    last_cta_size = 0
    kernel_ids = []
    kernel_total_cycles = []
    kernel_ipcs = []
    i = 0
    while (i < len(lines)): 
        if "launching kernel command : ./traces/kernel-" in lines[i]:
            #print("Found the line")
            #print(lines[i-1])
            i += 2
            if "CTA/core = " in lines[i]:
                wave_temp = int(lines[i].split()[4][:-1])
                last_cta_size = wave_temp
                #wave_sizes.append(int(line.split()[4][:-1]) * 80)
            else:
                wave_temp = last_cta_size
            #shaders = 1
            #i += 3
            #while("bind to kernel" in lines[i]):
            #    shaders += 1
            #    i += 2
            wave_sizes.append(wave_temp * 80)
        if "gpu_tot_issued_cta" in lines[i]:
            issued_ctas = int(lines[i].split()[-1])
            total_ctas.append(issued_ctas - prev_ctas)
            prev_ctas = issued_ctas
            i+=1
        if "kernel_name = " in lines[i]:
            kernel_names.append(lines[i].split()[-1])
            i += 1
        if "kernel id = " in lines[i]:
            kernel_ids.append(lines[i].split()[-1])
            i += 1
        if "gpu_sim_cycle" in lines[i]:
            kernel_total_cycles.append(lines[i].split()[-1])
            i += 1
        if "gpu_ipc" in lines[i]:
            kernel_ipcs.append(lines[i].split()[-1])
            i += 1
        i += 1
    kernel_info_dict = {}
    for kernel in range(len(kernel_names)):
        current_kernel = kernel_ids[kernel]
        kernel_info_dict[current_kernel] = {'kernel_names': kernel_names[kernel],
                                            'wave_sizes': wave_sizes[kernel],
                                            'total_ctas': total_ctas[kernel],
                                            'kernel_ids': kernel_ids[kernel],
                                            'kernel_ipc': kernel_ipcs[kernel],
                                            'total_cycles': kernel_total_cycles[kernel]}
    return kernel_info_dict

# The path assumes both .0.o and log gz files are located in the same directory
def main(path, return_format_type = "dict", output_log_format_type = 3):
    #print(path)
    log_path, out_path = get_paths(path)
    #print(out_path)
    try:
        if(return_format_type == "dict"):
            print("Parsing lines as Dictionary")
            return parse_lines_dictionary(log_path, out_path, output_log_format_type)
        else:
            return parse_lines(log_path, out_path, output_log_format_type)
    except Exception as e:
        print("----------------------------------")
        print("Some sort of error")
        print(e)
#        print(log_path)
#        print(out_path)
        print("----------------------------------")
        return []
    
def main_only_o_files(path):
    log_path, out_path = get_paths(path)
    return parse_only_output_file(out_path)

def intra_launch_insts(consecutive_regions, sim_data):
    ipc = []
    launch_instructions_simulated = 0
    launch_instructions_skipped = 0
    launch_cycles_simulated = 0
    launch_cycles_skipped = 0
    launch_IPC = 0
    total_cycles = np.sum([sim_data[i]['cycles'] for i in range(len(sim_data))])
    total_instructions = np.sum([sim_data[i]['instructions'] for i in range(len(sim_data))])

    #print(consecutive_regions)
    for region in consecutive_regions:
        start = region['start']
        end = region['end']-1
        interval = region['interval']
        if(interval == 0):
            launch_instructions_simulated += sim_data[start]['instructions']
            launch_cycles_simulated += float(sim_data[start]['cycles'])
            #ipc.append( sim_data[start]['instructions'] / sim_data[start]['cycles'] )
            #print(ipc[-1])
        else:
            homogeneous_ipc = sim_data[start]['instructions'] / sim_data[start]['cycles']
            launch_instructions_simulated += sim_data[start]['instructions']
            launch_cycles_simulated += float(sim_data[start]['cycles'])
            if(homogeneous_ipc == 0):
                #print(sim_data[start])
                print("Homogeneous IPC = 0")
                print(start)
                print(sim_data)
                print(sim_data[start]['instructions'])
                print(sim_data[start]['cycles'])
            region_data = [x['instructions'] for x in sim_data[start+1:end]]
            launch_instructions_skipped += np.sum(region_data)
            launch_cycles_skipped += np.sum(region_data) / homogeneous_ipc
    projected_ipc = (launch_instructions_simulated + launch_instructions_skipped)/(launch_cycles_simulated + launch_cycles_skipped)
    error = np.abs(projected_ipc - (total_instructions / total_cycles)) / (projected_ipc) * 100
    speedup = total_cycles / launch_cycles_simulated
    return {'projected_ipc': projected_ipc, 'actual_ipc': total_instructions / total_cycles, 'simulated_cycles': launch_cycles_simulated, 
            'speedup': speedup, 'error': error, 'total_cycles': total_cycles, 'consecutive': consecutive_regions}

def retopologize_data(simulator_data):
    retopologized_simulator_data = {}
    #kernel_number = str(2)
    for k in range(len(simulator_data[0])):
        retopod_simulator_data = []
        retodod_simulator_data_mem = []
        retodod_simulator_data_ctas = []
        retodod_simulator_data_instructions = []
        retodod_simulator_data_cycles = []

        kernel_number = str(k+1)
        j = 0
        i = 0
        last_cycles = 0
        last_instructions = 0
        if( float(simulator_data[1][kernel_number]['wave_sizes'])) > np.sum(np.array(simulator_data[0][kernel_number]['ctas'])):
            sum_ = np.sum(simulator_data[0][kernel_number]['ctas'])
            instructions_ = float(simulator_data[0][kernel_number]['instructions'][-1])
            cycles_ = float(simulator_data[0][kernel_number]['cycles'][-1])
            mem_ = np.sum(np.array(simulator_data[0][kernel_number]['mem_loads']).astype(float)) + np.sum(np.array(simulator_data[0][kernel_number]['mem_writes']).astype(float)) 
            retopod_simulator_data.append({'ctas': sum_, 'cycles': cycles_,'instructions': instructions_,
                                              'memory_requests': mem_})

        while(i < len(simulator_data[0][kernel_number]['ctas'])):            
        #for i in range(len(simulator_data[0]['1']['ctas'])):
            sum_ = np.sum(simulator_data[0][kernel_number]['ctas'][j:i])
            while(sum_ < simulator_data[1][kernel_number]['wave_sizes'] and i < len(simulator_data[0][kernel_number]['ctas'])):
                sum_ += simulator_data[0][kernel_number]['ctas'][i]
                i += 1
            if(i > len(simulator_data[0][kernel_number]['ctas'])):
                mem_ = np.sum(np.array(simulator_data[0][kernel_number]['mem_loads'][j:]).astype(float)) + np.sum(np.array(simulator_data[0][kernel_number]['mem_writes'][j:]).astype(float))
                cycles_ = float(simulator_data[0][kernel_number]['cycles'][i]) - last_cycles
                instructions_ = float(simulator_data[0][kernel_number]['instructions'][i]) - last_instructions
                retodod_simulator_data_mem.append(mem_)
                retodod_simulator_data_ctas.append(sum_)
                retodod_simulator_data_cycles.append(float(simulator_data[0][kernel_number]['cycles'][-1]))
                retodod_simulator_data_instructions.append(np.sum(np.array(simulator_data[0][kernel_number]['instructions'][j:]).astype(float)))
                retopod_simulator_data.append({'ctas': sum_, 'cycles': retodod_simulator_data_cycles[-1],'instructions': retodod_simulator_data_instructions[-1],
                                              'memory_requests': mem_})

            else:
                mem_ = np.sum(np.array(simulator_data[0][kernel_number]['mem_loads'][j:i]).astype(float)) + np.sum(np.array(simulator_data[0][kernel_number]['mem_writes'][j:i]).astype(float))
                cycles_ = float(simulator_data[0][kernel_number]['cycles'][i-1]) - last_cycles
                last_cycles = float(simulator_data[0][kernel_number]['cycles'][i-1])
                instructions_ = float(simulator_data[0][kernel_number]['instructions'][i-1]) - last_instructions
                last_instructions = float(simulator_data[0][kernel_number]['instructions'][i-1])
                retodod_simulator_data_mem.append(mem_)
                retodod_simulator_data_ctas.append(sum_)
                retodod_simulator_data_cycles.append(cycles_)
                retodod_simulator_data_instructions.append(np.sum(np.array(simulator_data[0][kernel_number]['instructions'][j:i]).astype(float)))
                retopod_simulator_data.append({'ctas': sum_, 'cycles': cycles_, 'instructions': instructions_,
                                               'memory_requests': mem_})
            #print("Found range")
            #print("["+str(j)+','+str(i)+']')
            #print(mem_)
            #print(cycles_)
            #print(instructions_)
            j = i
            retopologized_simulator_data[kernel_number] = retopod_simulator_data
    return retopologized_simulator_data

def find_consecutive_regions(X, clustering):
    consecutive_regions = []

    j = 0
    i = 0

    while(i < len(X)-1):
        if(clustering.labels_[i] == clustering.labels_[i+1]):
            j = i
            i += 1
            while(i < len(X)-1) and (clustering.labels_[i] == clustering.labels_[j]):
                i += 1
        else:
            i += 1
        consecutive_regions.append({'start':j,'end':i,'interval':i-j})
        j = i

    if(clustering.labels_[i] != clustering.labels_[i-1]):
        consecutive_regions.append({'start':j,'end':i,'interval':0})
    return consecutive_regions

def TBPoint_IntraKernel(simulator_data):
    retopo_data = retopologize_data(simulator_data)
    results = {}
    for kernel in retopo_data:
        X = [[retopo_data[kernel][i]['ctas'],float(retopo_data[kernel][i]['memory_requests']) / float(retopo_data[kernel][i]['instructions'] + 0.1) ] for i in range(len(retopo_data[kernel])) ]
        #X = [[retodod_simulator_data_ctas[i], retodod_simulator_data_mem[i] / retodod_simulator_data_instructions[i]] for i in range(len(retodod_simulator_data_ctas))]
        X_scaled = StandardScaler().fit_transform(X)
        clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.2).fit(X_scaled)
        consecutive_regions = find_consecutive_regions(X, clustering)
        #print(consecutive_regions)
        results[kernel] = intra_launch_insts(consecutive_regions,retopo_data[kernel])
    geomean_speed = scipy.stats.gmean([results[k]['speedup'] for k in results])
    mean_error = np.mean([results[k]['error'] for k in results])
    return {'results': results, 'geomean_speedup': geomean_speed, 'mean_error': mean_error}


# RKS --------------------------------------------------------------
# Because the nsight compute program doesn't have an exclusive
# export to CSV option, using the buffer stdout instead, we have to
# get rid of some lines, this function does that. 
def ignore_lines(file):
    # check the first 1000 lines, if the phrase "device__attribute_async_engine_count"
    # is found, else return -99
    # This phrase is always printed by nsight-compute.
    i = 0
    try:
        with open(file) as in_file:
            for line in in_file:
                if ("device__attribute_async_engine_count" in line):
                    return i
                elif(i > 1000):
                    return -99
                i += 1
            
# General open file function, takes in a nsight-compute csv filename, 
# returns a pandas table.
def open_file(filename):
    #print(filename)
    lines_to_ignore = ignore_lines(filename)
    if(lines_to_ignore == -99):
        lines_to_ignore = 0
    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)
    if(table.loc[0,'ID'] != 0.0):
        table = table.drop(0)
        table.index = range(len(table))
    # For certain systems nsight-compute prints
    # comma-deliniated numbers, e.g. 1,000 instead of 1000.
    # We need to get rid of that. 
    table = table.replace(',','', regex=True)
    return table

# List of agnostic features to take into consideration when performing
# the PCA (i.e. only these variables are going to be used)
agnostic_features = ['l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum',
                     'l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum',
                     #'l1tex__t_sectors_pipe_lsu_mem_local_op_ld.sum',
                     'smsp__inst_executed.sum','smsp__thread_inst_executed_per_inst_executed.ratio',
                     'smsp__sass_inst_executed_op_global_atom.sum',
                     'smsp__inst_executed_op_global_ld.sum',
                     'smsp__inst_executed_op_global_st.sum',
                     'smsp__inst_executed_op_shared_ld.sum',
                     'smsp__inst_executed_op_shared_st.sum',
                     #'smsp__inst_executed_op_surface_atom.sum',
                     #'smsp__inst_executed_op_surface_ld.sum',
                     #'smsp__inst_executed_op_surface_red.sum',
                     #'smsp__inst_executed_op_surface_st.sum',
                     #'smsp__sass_inst_executed_op_global_red.sum',
                     #'smsp__inst_executed_op_shared_atom.sum',
                     #'smsp__inst_executed_op_global_red.sum',
                     'sass__inst_executed_global_loads',
                     'sass__inst_executed_global_stores',
                     'launch__grid_size']

# Opens the cycles-profiled-only csvs generated by
# the nsight-compute utility, obtained using the 
# run_hw.py under the accel-sim utility.
def open_cycles(filename):
    #print(filename)
    lines_to_ignore = ignore_cycle_lines(filename)
    if(lines_to_ignore == -99):
        lines_to_ignore = 0
    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)
    return table

def ignore_cycle_lines(file):
    # check the first 1000 lines, if the phrase "Section Name" is not found
    # return -99. Same as ignore_lines above, but for the cycles-only version.
    i = 0
    with open(file) as in_file:
        for line in in_file:
            if ("Section Name" in line):
                return i
            elif(i > 1000):
                return -99
            i += 1

def generate_DF(table, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID']):
    x_large = table.loc[:,agnostic_features].values
    x_large = StandardScaler().fit_transform(x_large)
    temp = table.loc[:,agnostic_features]
    if(True):
        for value in agnostic_features:
            if(temp[value].isnull().values.any()):
                print(value)
    for new_col in appendable_columns:
        temp = table[[new_col]]
        if "Name" not in new_col:
            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))
        try:
            x_large = pd.concat([x_large, temp], axis = 1)
        except:
            x_large = pd.concat([pd.DataFrame(x_large), pd.DataFrame(temp)], axis = 1)
    return x_large

def generate_PCA(table, pca_variation=0.9995, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID'], debug=False):
    x_large = table.loc[:,agnostic_features].values
    #x_large = x_large.apply(lambda x: pd.to_numeric(x.astype(str).str.replace(',','')))
    #x_large = x_large.values
    x_large = StandardScaler().fit_transform(x_large)
    temp = table.loc[:,agnostic_features]
    if(True):
        for value in agnostic_features:
            if(temp[value].isnull().values.any()):
                print(value)
    pca_components = pca_variation
    pca = PCA(pca_components)
    principalComponents_Large = pca.fit_transform(x_large)
    principalDf_Large = pd.DataFrame(data = principalComponents_Large, columns = ['principal component '+str(x+1) for x in range(pca.n_components_)])
    for new_col in appendable_columns:
        temp = table[[new_col]]
        if "Name" not in new_col:
            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))
        principalDf_Large = pd.concat([principalDf_Large, temp], axis = 1)
    #finalDf_Large = pd.concat([finalDf_Large,truncated_names], axis = 1)
    #finalDf_Large = pd.concat([finalDf_Large, nsight_csv[['ID']]], axis = 1)
    #finalDf_sorted = finalDf_Large#.sort_values('sm__inst_issued.avg.per_cycle_active [inst/cycle]')
    #%matplotlib inline
    #fig = plt.figure(figsize = (10,10))
    #ax = fig.add_subplot(111) 
    #ax.set_title('PCA components - variance', fontsize = 20)
    #plt.bar(range(pca.n_components_), pca.explained_variance_ratio_, color='black')
    #plt.xlabel('PCA features')
    #plt.ylabel('variance %')
    #plt.xticks(range(pca.n_components_))
    #plt.show()
    # Clustering
    #k_means = KMeans(n_clusters=2, random_state=2).fit(principalComponents_Large)
    #finalDf_sorted['Segments'] = k_means.labels_
    #print(principalDf_Large)
    return principalDf_Large, principalComponents_Large 

# Naively choosing one from each group at random
def kmeans_clustering(dataFrame,principalComponents_dataFrame,starting_from=1, ending_at=20, column_variable='gpc__cycles_elapsed.avg',print_output=False):
    total_runtime = dataFrame[column_variable].sum()
    complete_groups = []
    random_per_K = []
    if(ending_at > len(dataFrame)):
        ending_at = len(dataFrame)+1
    results = {'random_choices_projections':[], 'random_choices_vectors':[], 'random_choices_names':[], 
               'random_choices_id':[], 'random_choices_speedups':[],'first_choices_projections':[],
               'center_choices_projections':[], 'center_choices_vectors':[], 'center_choices_names':[],
               'center_choices_id':[], 'center_choices_speedups':[], 'center_choices_errors': [],
               'complete_groups':[],
               'first_choices_vectors':[], 'first_choices_names':[], 'first_choices_id':[], 'group_count':[], 
               'total_runtime':total_runtime, 'group_number': [], 'errors': [], 'speedups': [], 'number_of_kernels': []}
    for i in range(starting_from, ending_at):
        random_choices = [] # Randomly select a kernel from the group
        random_choices_name = []
        random_choices_id = []
        mean_choices = []   # Select the kernel with the value closest to the mean
        max_choices = []    # Select the largest kernel value
        first_choices = []   # Selects the first chronological value
        first_choices_name = []
        first_choices_id = []
        center_choices_id = []
        closest_to_mean = []
        closest_to_mean_names = []
        group_count = []    # Number of elements inside the cluster i
        complete_groups_df = []
        k_means = KMeans(n_clusters=i, random_state=4).fit(principalComponents_dataFrame)
        dataFrame['Segments'] = k_means.labels_
        center_ids = cluster_centers(dataFrame,principalComponents_dataFrame,k_means)
        per_group_random = []
        for group in np.unique(k_means.labels_):
            temp_df = dataFrame.loc[dataFrame['Segments'] == group]
            #closest_to_mean.append(temp_df.loc[center_ids[group], column_variable])
            #closest_to_mean_names.append(temp_df.loc[center_ids[group], 'Kernel Name'])
            complete_groups_df.append(temp_df)
            temp_df.index = range(len(temp_df))
            #value_first = temp_df.loc[0,column_variable]
            first_choices.append(temp_df.loc[0,column_variable])
            first_choices_name.append(temp_df.loc[0,'Kernel Name'])
            first_choices_id.append(temp_df.loc[0,'ID'])
            #center_choices_id.append(center_ids[group])
            temp_df_sorted = temp_df.sort_values(column_variable)
            temp_df_sorted.index = range(len(temp_df_sorted))
            group_count.append(len(temp_df))
            random_vals = []
            random_names = []
            random_ids = []
            random_choice = random.randint(0,len(temp_df)-1)
            random_choice_name = ''
            for i in range(10):
                random_choice_ = random.randint(0,len(temp_df))
                random_choice_name = ''
                try:
                    random_vals.append(temp_df_sorted.loc[random_choice_,column_variable] * len(temp_df))
                    random_names.append(temp_df_sorted.loc[random_choice, 'Kernel Name'])
                    random_ids.append(random_choice_)
                except:
                    random_vals.append(temp_df_sorted.loc[0,column_variable] * len(temp_df))
                    random_names.append(temp_df_sorted.loc[0, 'Kernel Name'])
                    random_ids.append(random_choice)
            max_choice = temp_df_sorted.loc[len(temp_df)-1, column_variable]
            try:
                value_mean = temp_df_sorted.loc[int(len(temp_df)/2),column_variable]
            except:
                print(int(len(temp_df)/2))
            try:
                value_random = temp_df_sorted.loc[random_choice,column_variable]
                random_choice_name = temp_df_sorted.loc[random_choice, 'Kernel Name']
            except:
                print("Why would this happen?")
                print("Random Choice: "+str(random_choice))
                print("Length dataFrame: "+str(len(temp_df_sorted)))
                value_random = temp_df_sorted.loc[0,column_variable]
                random_choice_name = temp_df_sorted.loc[0,'Kernel Name']
            #value_random = 0
            random_choices.append(value_random)
            random_choices_id.append(random_choice)
            random_choices_name.append(random_choice_name)
            mean_choices.append(value_mean)
            max_choices.append(max_choice)
            per_group_random.append({'random_vals': random_vals, 'random_names': random_names, 'random_ids': random_ids})
        complete_groups.append(complete_groups_df)
        random_runtime = [random_choices[i] * group_count[i] for i in range(len(random_choices))]
        mean_runtime = [mean_choices[i] * group_count[i] for i in range(len(random_choices))]
        max_runtime = [max_choices[i] * group_count[i] for i in range(len(random_choices))]
        first_runtime = [first_choices[i] * group_count[i] for i in range(len(random_choices))]
        #closest_runtime = [closest_to_mean[i] * group_count[i] for i in range(len(random_choices))]
        random_per_K.append(per_group_random)
        if(print_output):
            print('For '+str(i)+ ' groups')
            print('----------------------------------------------')
            print(dataFrame[column_variable])
            print('The actual run time is: '+str(total_runtime))
            print('----------------- Projections ----------------')
            print(' ')
            print('The random value run time is: '+str(np.sum(random_runtime)))
            print('The error is '+ str(np.sum(random_runtime) / total_runtime ))
            print('The mean value run time is: '+str(np.sum(mean_runtime)))
            print('The error is '+ str(np.sum(mean_runtime) / total_runtime ))
            print('The max value run time is: '+str(np.sum(max_runtime)))
            print('The error is '+ str(np.sum(max_runtime) / total_runtime ))
            print('The first value run time is: '+str(np.sum(first_runtime)))
            print('The error is '+ str(np.sum(first_runtime) / total_runtime ))
            print('The speedup is '+ str(total_runtime / np.sum(first_choices)))
            print('The reduced number of kernels '+str(len(first_runtime)))
            print('The total number of kernels '+str(len(dataFrame)))
            print('The first choice vector is '+str(first_choices))
            print('The number of elements per group is '+str(group_count))
            print('The names of the first choices are '+str(first_choices_name))
            print('The kernel IDs of the first choices are '+str(first_choices_id))
        #print('Their product is '+str( [first_choices[i] * group_count[i] for i in range(len(first_choices))] ))
            print(' ')
        results['errors'].append(np.abs((np.sum(first_runtime)-total_runtime)) / total_runtime)
        results['speedups'].append(total_runtime / np.sum(first_choices))
        results['first_choices_vectors'].append(first_choices)
        results['first_choices_projections'].append(first_runtime)
        results['first_choices_names'].append(first_choices_name)
        results['first_choices_id'].append(first_choices_id)
        results['random_choices_vectors'].append(random_choices)
        results['random_choices_projections'].append(random_runtime)
        results['random_choices_names'].append(random_choices_name)
        results['random_choices_id'].append(random_choices_id)
        #results['center_choices_projections'].append(closest_runtime)
        #results['center_choices_id'].append(center_choices_id)
        #results['center_choices_vectors'].append(closest_to_mean)
        #results['center_choices_names'].append(closest_to_mean_names)
        #results['center_choices_errors'].append(np.abs((total_runtime - np.sum(closest_runtime))) / total_runtime)
        results['group_count'].append(group_count)
        results['group_number'].append(group)
        results['complete_groups'].append(complete_groups)
        results['number_of_kernels'].append(len(dataFrame))
        results['random_per_K'] = random_per_K
        #print(finalDf_sorted.loc[finalDf_sorted['Segments'] == group])
    return results
#kmeans_clustering(finalDf_sorted, principalComponents_Large)

# Returns the ID of the element closest to the group's centers
def cluster_centers(dataFrame, principalComponents_dataFrame, k_means, debug=False):
    centers_vector = []
    centers_ids = []
    clusters_centers = k_means.cluster_centers_
    temp_pca_df = dataFrame.copy()
    #labels = ['principal component '+str(x+1) for x in range(len(np.unique(k_means.labels_)))]
    for group in np.unique(k_means.labels_):
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_grouped_pca_df = temp_df.filter(regex='principal')
        original_indeces = temp_grouped_pca_df.index.values.tolist()
        temp_grouped_pca_df.index = range(len(temp_grouped_pca_df))
        minimum_distance = temp_grouped_pca_df.loc[0,:].values.tolist()
        minimum_index = original_indeces[0]
        #print(minimum_distance)
        if(debug):
            print('The clusters centers are: '+str(clusters_centers[group]))
            print('The first data point is : '+str(minimum_distance))
        minimum_distance = distance.euclidean(minimum_distance, clusters_centers[group])
        if(debug):
            print(minimum_distance)
        for i in range(len(temp_grouped_pca_df)):
            data_point = temp_grouped_pca_df.loc[i,:].values.tolist()
            temp_distance = distance.euclidean(data_point, clusters_centers[group])
            if(temp_distance < minimum_distance):
                minimum_distance = temp_distance
                minimum_index = original_indeces[i]
        if(debug):
            print(minimum_distance, minimum_index)
        centers_ids.append(minimum_index)
    if(debug):
        print('Returns from function \nNew iteration\n\n')
    return centers_ids

def point_closest_to_all(dataFrame, principalComponents_dataFrame, k_means, debug=False):
    closest_ids = []
    for group in np.unique(k_means.labels_):
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_grouped_pca_df = temp_df.filter(regex='principal')
        original_indeces = temp_grouped_pca_df.index.value.tolist()
    
def check_results(dataFrame, best_choices, group_counts, column_variable='gpc__cycles_elapsed.avg',print_debug=True):
    result_dictionary = {}
    total_runtime = pd.to_numeric(dataFrame[column_variable]).sum()
    runtime_list = []
    projected_runtime = 0.0
    for i in range(len(best_choices)):
        index = int(best_choices[i])
        runtime_list.append(float(dataFrame.loc[index, column_variable]))
        projected_runtime += float(dataFrame.loc[index, column_variable]) * group_counts[i]
    if(print_debug):
        print('The actual runtime is: '+str(total_runtime))
        print('The projected runtime with the borrowed K-Means is: '+str(projected_runtime))
        print('The ratio is '+str(projected_runtime/total_runtime))
    result_dictionary = {'total_runtime': total_runtime, 'projected_runtime': projected_runtime,
                         'error': np.abs(projected_runtime - total_runtime)/total_runtime,
                         'speedup': total_runtime / np.sum(runtime_list), 'reduced_runtime': np.sum(runtime_list), 
                         'runtime_list': runtime_list}
    return result_dictionary 

def open_all(profile_paths):
    Beeg_Table = {}
    list_of_apps = []
    for app in profile_paths:
        try:
            print(app)
            table = open_file(profile_paths[app])
            DF_Large, PC_Large = generate_PCA(table)
            results = kmeans_clustering(DF_Large, PC_Large)
            Beeg_Table[app] = results
            list_of_apps.append(app)
        except Exception as e:
            print(app)
            print(e)
            print("Has a problem")
    return Beeg_Table, list_of_apps

def process_all(Beeg_Table, list_of_apps, extra_archs=[''], min_error_arg = 0.05, all_paths_ampere = [], all_paths_turing = []):
    list_of_speedups = []
    list_of_errors = []
    best_choices_dict = {}
    dict_best_choices = {}
    for app in list_of_apps:
        min_error = 99
        index_min_error = 99
        min_error_speedup = 99
        group_counts_min = []
        kernel_ids_min = []
        first_error_less_than_4_percent = []
        first_error_less_than_3_percent = []
        first_error_less_than_2_percent = []
        for i in range(len(Beeg_Table[app]['errors'])):
            error = Beeg_Table[app]['errors'][i]
            #error = Beeg_Table[app]['center_choices_errors'][i]
            speedup = Beeg_Table[app]['speedups'][i]
            group_counts = Beeg_Table[app]['group_count'][i]
            #kernel_ids = Beeg_Table[app]['center_choices_id'][i]
            kernel_ids = Beeg_Table[app]['first_choices_id'][i]
            print("The error is: "+str(error))
            print("The speedup is: "+str(speedup))
            if(error < min_error):
                min_error = error
                index_min_error = i
                min_error_speedup = speedup
                group_counts_min = group_counts
                kernel_ids_min = kernel_ids
            if(error < min_error_arg):
                first_error_less_than_4_percent.append([error,i,speedup, group_counts, kernel_ids])
                if(error < min_error_arg/2.0):
                    first_error_less_than_3_percent.append([error,i,speedup])
                    if(error < min_error_arg/4.0):
                        first_error_less_than_2_percent.append([error,i,speedup])
        if(len(first_error_less_than_4_percent)==0):
            first_error_less_than_4_percent.append([min_error, index_min_error, min_error_speedup,group_counts_min, kernel_ids_min])
        list_of_speedups.append(first_error_less_than_4_percent[0][2])
        list_of_errors.append(first_error_less_than_4_percent[0][0])
        best_choices_dict[app] = [first_error_less_than_4_percent[0][4],
                                  first_error_less_than_4_percent[0][3],
                                  first_error_less_than_4_percent[0][2]]
        dict_best_choices[app] = {'kernel_ids':first_error_less_than_4_percent[0][4], 
                                  'group_counts': first_error_less_than_4_percent[0][3], 
                                  'speedup': first_error_less_than_4_percent[0][2], 
                                  'error': first_error_less_than_4_percent[0][0]}

        #print(best_choices_dict[app])

    extra_archs_results = {}
    turing_results = {}
    if('turing' in extra_archs):
        list_of_turing_apps = []
        list_of_turing_results = []
        list_of_turing_speedups = []
        list_of_turing_names = []
        list_of_turing_total_runtimes = []
        for app in all_paths_turing:
            #print(table)
            try:
                table = open_file(all_paths_turing[app])
                time = pd.to_numeric(table['gpc__cycles_elapsed.avg']).sum()
                DF_Large = generate_DF(table)
                list_of_turing_apps.append(app)
                temp_results = check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1])
                turing_results[app] = temp_results
                list_of_turing_results.append(temp_results)
                list_of_turing_speedups.append(np.sum(best_choices_dict[app][0])/temp_results[0])
                list_of_turing_names.append(app)
                list_of_turing_total_runtimes.append(time)
            except Exception as e:
                print(e)
                pass
        extra_archs_results['turing'] = turing_results
#        extra_archs_results['turing'] = {'results': list_of_turing_results, 'speedups': list_of_turing_speedups, 'names': list_of_turing_names, 'total_time':list_of_turing_total_runtimes}
            #print(check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1]))
            
    if('ampere' in extra_archs):
        list_of_ampere_apps = []
        list_of_ampere_results = []
        list_of_ampere_speedups = []
        list_of_ampere_names = []
        for app in all_paths_ampere:
            try:
                table = open_file(all_paths_ampere[app])
                DF_Large = generate_DF(table)
                list_of_ampere_apps.append(app)
                temp_results = check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1])
                list_of_ampere_results.append(temp_results)
                list_of_ampere_speedups.append(temp_results[0]/np.sum(best_choices_dict[app][0]))
                list_of_ampere_names.append(app)
            except:
                pass
        extra_archs_results['ampere'] = {'results': list_of_ampere_results, 'speedups': list_of_ampere_speedups, 'names': list_of_ampere_names}

    return best_choices_dict, list_of_errors, list_of_speedups, dict_best_choices, extra_archs_results

def kernel_distribution_per_kmean_groups(dataFrame,principalComponents_dataFrame,best_choice=5, kernel_name = 'Kernel Name'):
    #for i in range(starting_from, ending_at):
    histogram_cu = {}
    histogram_fn = {}
    total_runtimes = []
    k_means = KMeans(n_clusters=best_choice, random_state=4).fit(principalComponents_dataFrame)
    dataFrame['Segments'] = k_means.labels_
    print(np.unique(k_means.labels_))
    unique_cufunction_names = dataFrame[kernel_name].unique()
    unique_kernel_names = dataFrame[kernel_name].unique()
    for group in np.unique(k_means.labels_):
        for key in unique_cufunction_names:
            histogram_cu[key] = 0
        for key in unique_kernel_names:
            histogram_fn[key] = 0
        # Extract only the kernels related to the group inside the loop
        fig = plt.figure(figsize = (20,10))
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_df.index = range(len(temp_df))
        runtime = temp_df['gpc__cycles_elapsed.avg'].sum()
        total_count = len(temp_df)
        for i in range(len(temp_df)):
            cu_name = temp_df.loc[i, kernel_name]
            func_name = temp_df.loc[i, kernel_name]
            histogram_cu[cu_name] += 1
            histogram_fn[func_name] += 1
        total_runtimes.append(runtime)
        ax = fig.add_subplot(121)
        ax.set_title('K-Means Group '+str(group),fontsize=30)
        chart = plt.bar(histogram_cu.keys(), histogram_cu.values(), 1)
        ax.legend(['Kernel count: '+str(total_count)+'\nTotal Runtime: '+f"{runtime:,}"], fontsize=20)
        ax.set_xticklabels(histogram_cu.keys(),rotation=90)
        ax = fig.add_subplot(122)
        chart = plt.bar(histogram_fn.keys(), histogram_fn.values(), 1)
        ax.set_xticklabels(histogram_fn.keys(),rotation=90)
    fig = plt.figure(figsize = (20,10))
    ax = fig.add_subplot(111)
    ax.set_title("Total Runtimes as a function of Group Id", fontsize=30)
    plt.bar(range(len(total_runtimes)), total_runtimes)
    plt.xlabel('Group ID',fontsize=20)
    plt.ylabel('Run time',fontsize=20)
        #temp_df['CUfunction'].hist()
        
def cycles_rolling_mean_method_with_ctas(kernel_list, kernel_info, kernel='0', window=2,threshold=0.1, cta_wave_threshold = 0.5, debug = False):    
    if (kernel_info[kernel]['wave_sizes'] > kernel_info[kernel]['total_ctas']):
        ctas_threshold = 0.3 * cta_wave_threshold * kernel_info[kernel]['total_ctas']
    else:
        ctas_threshold = cta_wave_threshold * kernel_info[kernel]['wave_sizes']
    if (not (kernel in kernel_list.keys())):
        print("Kernel not found")
        ctas = kernel_info[kernel]['total_ctas']
        cycles_so_far = 0
        total_cycles = 0
        if (kernel not in kernel_list.keys()):
            cycles_so_far = kernel_info[kernel]['total_cycles']
            total_cycles = kernel_info[kernel]['total_cycles']
        else:
            cycles_so_far = kernel_list[kernel]['cycles'][-1]
            total_cycles = kernel_list[kernel]['cycles'][-1]
        if(debug):
            print('Wave much larger than total number of CTAS, skipping')
            print(kernel_info[kernel]['wave_sizes'])
            print(kernel_info[kernel]['total_ctas'])
        error = 0
        speedup = 1
        projection_cycles = cycles_so_far
        cycles_remaining = 0
        total_ctas = ctas
        return {'current_ctas': ctas, 'current_cycles': cycles_so_far, 'total_cycles': total_cycles, 'error': error, 'speedup': speedup,
                      'projection_cycles': total_cycles, 'total_ctas': total_ctas, 'min_index': -1}
    print(kernel_list[kernel].keys())
    try:
        series_full = DataFrame(kernel_list[kernel])
    except:
        series_full = DataFrame({a: kernel_list[kernel][a] for a in ['cycles', 'instructions', 'ctas', 'ipcs']})
    temp_std = series_full['ipcs'].rolling(window).std()
    temp_mean = series_full['ipcs'].rolling(window).mean()
    ctas = np.array([np.sum(kernel_list[kernel]['ctas'][:i]) for i in range(len(kernel_list[kernel]['ctas']))])
    temp_std = temp_std.divide(temp_mean) * 100.0
    
    #ctas_threshold = kernel_info[kernel]  ['wave_sizes'] 
    #plt.figure(figsize=(20,20))
    #plt.plot(range(len(temp_std)), temp_std)
    #plt.hlines(threshold, 0,len(temp_std))
    
    #plt.figure(figsize=(20,20))
    #plt.plot(range(len(ctas)), ctas)
    #plt.hlines(ctas_threshold, 0,len(ctas))
    
    min_indexes_ctas = np.where(ctas >= ctas_threshold)
    min_indexes_threshold = np.where((temp_std < threshold))
    #print("ctas:")
    #print(min_indexes_ctas)
    #print('ipcs:')
    #print(min_indexes_threshold)
    if(debug):
        print("min indexes threshold: " +str(min_indexes_threshold))
    if(len(min_indexes_threshold[0]) == 0 or len(min_indexes_ctas[0]) == 0):
        ctas = kernel_info[kernel]['total_ctas']
        cycles_so_far = kernel_list[kernel]['cycles'][-1]
        total_cycles = kernel_list[kernel]['cycles'][-1]
        error = 0
        speedup = 1
        projection_cycles = kernel_list[kernel]['cycles'][-1]
        cycles_remaining = 0
        total_ctas = ctas
        return {'current_ctas': ctas, 'current_cycles': cycles_so_far, 'total_cycles': total_cycles, 'error': error, 'speedup': speedup,
                      'projection_cycles': total_cycles, 'total_ctas': total_ctas, 'min_index': -1}
    
    min_index_ctas = min_indexes_ctas[0][0]
    min_index_threshold = min_indexes_threshold[0][0]
    min_index = min_index_ctas if min_index_ctas > min_index_threshold else min_index_threshold
    if(debug):
        print("The min index for threshold is " +str(min_indexes_threshold[0][0]))
        print("The chosen min-index for threshold is " +str(min_index))
        print("The number of cycles at minindex is " + str(kernel_list[kernel]['cycles'][min_index]))
    total_ctas = kernel_info[kernel]['total_ctas']
    current_ctas = ctas[min_index]
    cycles_so_far = kernel_list[kernel]['cycles'][min_index]
    total_cycles = kernel_list[kernel]['cycles'][-1]
    ctas_completion_ratio = total_ctas / (current_ctas * 1.0 + 1)
    cycles_remaining = ctas_completion_ratio * cycles_so_far
    error = (cycles_remaining - total_cycles) / (total_cycles * 1.0) * 100.0
    speedup = total_cycles / cycles_so_far 
    if(debug):
        print(cycles_remaining)
        print("CTAs ratio "+ str(ctas_completion_ratio)  )
        print("Total Cycles: " +str(total_cycles))
        print("Current cycles: "+str(cycles_so_far))
        print("Projected cycles remaining: "+str(cycles_remaining))
        print("Actual cycles remaining: "+str(total_cycles - cycles_so_far))
        print("Error: " +str( error ))
        print("Total ctas:" + str(total_ctas))
        print("Current ctas:" + str(current_ctas))
        print("The number of data points at this kernel is " + str(len(kernel_list[kernel]['cycles'])))
    temp_mean = series_full['ipcs'][:min_index].rolling(window).mean()
    temp_std = series_full['ipcs'][:min_index].rolling(window).std()
    return {'current_ctas': ctas[min_index], 'current_cycles': cycles_so_far, 'total_cycles': total_cycles, 'error': error, 'speedup': speedup,
                      'projection_cycles': cycles_remaining, 'total_ctas': total_ctas, 'min_index': min_index}

def projection_cycles_only_selected(kernel_list, kernel_info, PKS_dictionary = None, window=3, threshold=3, cta_wave_threshold=1):
    projection_results = {}
    mean_error = {}
    mean_speedup = {}
    big_total_speedup_numerator = 0.0
    big_total_speedup_denominator = 0.0
    cycles_original_ = []
    cycles_speedup_ = []
    cycles_reduced_ = []
    cycles_projection_ = []
    cycles_error_ = []
    current_ctas_ = []
    total_ctas_ = []
    if(PKS_dictionary):
        kernel_ids = PKS_dictionary[app]['kernel_ids']
    else:
        kernel_ids = kernel_list.keys()
    for kernel in kernel_ids:
            if(PKS_dictionary):
                k = str(int(kernel) + 1)
            else:
                k = kernel
            result_dict = cycles_rolling_mean_method_with_ctas(kernel_list, kernel_info, kernel=k, window=window, threshold=threshold,cta_wave_threshold=cta_wave_threshold,debug=False)
            projection_results[k] = {'cycles_original': result_dict['total_cycles'],
                                          'speedup':result_dict['speedup'], 
                                          'cycles_reduced':result_dict['current_cycles'],
                                          'cycles_projection': result_dict['projection_cycles'],
                                          'error': result_dict['error'], 'current_ctas': result_dict['current_ctas'],
                                          'total_ctas': result_dict['total_ctas'], 'min_index': result_dict['min_index']}
    return projection_results

def projection_cycles(kernel_list, kernel_info, method="Mean with CTA", window=3, threshold=3, cta_wave_threshold = 1):
    projection_results = {}
    mean_error = {}
    mean_speedup = {}
    big_total_speedup_numerator = 0.0
    big_total_speedup_denominator = 0.0
    cycles_original_ = []
    cycles_speedup_ = []
    cycles_reduced_ = []
    cycles_projection_ = []
    cycles_error_ = []
    current_ctas_ = []
    total_ctas_ = []
    #print(sorted(list(kernel_info.keys())))
    for kernel in sorted(list(kernel_info.keys())):
        if(method == 'Financial'):
            pass
        elif(method == 'Mean with CTA'):
            result_dict = cycles_rolling_mean_method_with_ctas(kernel_list, kernel_info, kernel=kernel, window=window, threshold=threshold,cta_wave_threshold=cta_wave_threshold,debug=False)
            projection_results[kernel] = {'cycles_original': result_dict['total_cycles'],
                                          'speedup':result_dict['speedup'], 
                                          'cycles_reduced':result_dict['current_cycles'],
                                          'cycles_projection': result_dict['projection_cycles'],
                                          'error': result_dict['error'], 'current_ctas': result_dict['current_ctas'],
                                          'total_ctas': result_dict['total_ctas']}
        else:
            pass
    return projection_results

import os

def parse_cycles_per_kernel(output_path):
    with open(output_path, 'r') as f:
        cycles_per_kernel = []
        last_kernel_id = []
        result = {}
        for line in f:
            if "kernel id" in line:
                #print(line)
                last_kernel_id.append(line.split()[3])
            if "gpu_sim_cycle" in line:
                #print("found it")
                cycles_per_kernel.append(line.split()[2])
    for i in range(len(cycles_per_kernel)):
        result[last_kernel_id[i]] = cycles_per_kernel[i]
    return result
rawsult = {}
def process_all_files_discrete(RKS_Best_Choices_Dict, paths, single_app = 'BERT'):
    result_dict = {}
    for app in paths:
        output = {}
        output = {}
        output_2 = {}
        cycles_simulator = 0.0
        cycles_simulator_array = []
        best_kernels = RKS_Best_Choices_Dict[app]['kernel_ids']
        group_counts = RKS_Best_Choices_Dict[app]['group_counts']
        print(best_kernels)
        list_of_kernel_cycles = []
        #print(paths[app])
        for file in paths[app]:
            print(file)
            log_path, out_path = RGS.get_paths(file)
            #print(out_path)
            result = parse_cycles_per_kernel(out_path)
            print(log_path)
            print(out_path)
            kl, ki = parse_lines_dictionary(log_path, out_path)
            #print(ki)
            result_2 = projection_cycles(kl, ki,threshold=0.1)
            #print(ki['kernel_ids'])
            output.update(result)
            #print(result_2)
            output_2.update(result_2)
            #print(result)
        aggregate_result = 0.0
        reduced_cycles = 0.0
        projection_pka = 0.0
        projection_pks = 0.0
        for i in range(len(best_kernels)):
            k = str(int(best_kernels[i] + 1))
            #print(output[k])
            #print(group_counts[i])
            try:
                if("resnet_50_64b_4000" in app):
                    print(k)
                    print(output[k])
                cycles_simulator += output_2[k]['cycles_original']
                cycles_simulator_array.append(output_2[k]['cycles_original'])
                reduced_cycles += output_2[k]['cycles_reduced']
                print("For selected kernel " + str(k) + " , the group count is " + str(group_counts[i]))
                print("Simulating to totality takes " + str(output_2[k]['cycles_original']) + 'cycles')
                print("Partially simulating takes " + str(output_2[k]['cycles_reduced']) + 'cycles')
                print("The projected number of cycles is " + str(output_2[k]['cycles_projection']) + 'cycles')
                projection_pka += float(output_2[k]['cycles_projection']) * float(group_counts[i])
                projection_pks += float(output_2[k]['cycles_original']) * float(group_counts[i])
                aggregate_result += float(output_2[k]['cycles_projection']) * float(group_counts[i])
                #cycles_simulator += float(output[k])
            except:
                print(app)
                print(k)
                print('not found')
                pass
        result_dict[app] = {'projected_cycles_pks': projection_pks, 'projected_cycles': aggregate_result, 'reduced_cycles': reduced_cycles, 'simulator_cycles': cycles_simulator, 'simulator_cycles_array': cycles_simulator_array}
        rawsult[app] = output_2
        print(result_dict[app])
    return result_dict

# Tables opened
def process_all_files_discrete_opened_tables(RKS_Best_Choices_Dict, tables, single_app = 'BERT'):
    result_dict = {}
    for app in tables:
        output = {}
        output = {}
        output_2 = {}
        cycles_simulator = 0.0
        cycles_simulator_array = []
        try:
            best_kernels = RKS_Best_Choices_Dict[app]['kernel_ids']
            group_counts = RKS_Best_Choices_Dict[app]['group_counts']
            print(best_kernels)
            list_of_kernel_cycles = []
            kl = tables[app][0]
            ki = tables[app][1]
        except:
            continue
        result_2 = projection_cycles(kl, ki)
            #print(ki['kernel_ids'])
        #output.update(result)
            #print(result_2)
        output_2.update(result_2)
            #print(result)
        aggregate_result = 0.0
        projection_pks = 0.0
        projection_pka = 0.0
        reduced_cycles = 0.0
        total_runtime = 0.0
# (turing_simulations_full_dict['b'][0])['1']['cycles'][-1]
        for kernel in kl:
            total_runtime += kl[kernel]['cycles'][-1]
        for i in range(len(best_kernels)):
            k = str(int(best_kernels[i] + 1))
            #print(output[k])
            #print(group_counts[i])
            try:
                if("resnet_50_64b_4000" in app):
                    print(k)
                    print(output[k])
                cycles_simulator += output_2[k]['cycles_original']
                cycles_simulator_array.append(output_2[k]['cycles_original'])
                reduced_cycles += output_2[k]['cycles_reduced']
                print("For selected kernel " + str(k) + " , the group count is " + str(group_counts[i]))
                print("Simulating to totality takes " + str(output_2[k]['cycles_original']) + ' cycles')
                print("Partially simulating takes " + str(output_2[k]['cycles_reduced']) + ' cycles')
                print("The projected number of cycles is " + str(output_2[k]['cycles_projection']) + ' cycles')
                print("The total runtime is " + str(total_runtime) + " cycles ")
                projection_pka += float(output_2[k]['cycles_projection']) * float(group_counts[i])
                projection_pks += float(output_2[k]['cycles_original']) * float(group_counts[i])
                #cycles_simulator += float(output[k])
            except:
                print(app)
                print(k)
                print('not found')
                pass
        result_dict[app] = {'total_runtime': total_runtime, 'projected_cycles': projection_pka, 'projected_cycles_pka': projection_pka, 'projected_cycles_pks': projection_pks, 'reduced_cycles': reduced_cycles, 'simulator_cycles': cycles_simulator, 'simulator_cycles_array': cycles_simulator_array}
        rawsult[app] = output_2
        print(result_dict[app])
    return result_dict

# Perform PKA on the app
def process_simulation_files(RKS_Best_Choices_Dict, paths, single_app = 'BERT'):
    result_dict = {}
    for app in paths:
        output = {}
        output = {}
        output_2 = {}
        cycles_simulator = 0.0
        cycles_simulator_array = []
        best_kernels = RKS_Best_Choices_Dict[app]['kernel_ids']
        group_counts = RKS_Best_Choices_Dict[app]['group_counts']
#        print(best_kernels)
        list_of_kernel_cycles = []
        #print(paths[app])
#        print(file)
        log_path, out_path = RGS.get_paths(paths[app])
#        print(out_path)
        try:
            result = parse_cycles_per_kernel(out_path)
            kl, ki = parse_lines_dict(log_path, out_path)
            result_2 = projection_cycles(kl, ki)
            output.update(result)
            output_2.update(result_2)
        except Exception as e:
            print(e)
            print(paths[app])
            print(log_path)
            print(out_path)
            continue
        aggregate_result = 0.0
        reduced_cycles = 0.0
        for i in range(len(best_kernels)):
            k = str(int(best_kernels[i] + 1))
            #print(output[k])
            #print(group_counts[i])
            try:
                if("resnet_50_64b_4000" in app):
                    print(k)
                    print(output[k])
                cycles_simulator += output_2[k]['cycles_original']
                cycles_simulator_array.append(output_2[k]['cycles_original'])
                reduced_cycles += output_2[k]['cycles_reduced']
                #print("For selected kernel " + str(k) + " , the group count is " + str(group_counts[i]))
                #print("Simulating to totality takes " + str(output_2[k]['cycles_original']) + 'cycles')
                #print("Partially simulating takes " + str(output_2[k]['cycles_reduced']) + 'cycles')
                #print("The projected number of cycles is " + str(output_2[k]['cycles_projection']) + 'cycles')
                aggregate_result += float(output_2[k]['cycles_projection']) * float(group_counts[i])
                #cycles_simulator += float(output[k])
            except:
                print(app)
                print(k)
                print('not found')
                pass
        result_dict[app] = {'projected_cycles': aggregate_result, 'reduced_cycles': reduced_cycles, 'simulator_cycles': cycles_simulator, 'simulator_cycles_array': cycles_simulator_array}
        rawsult[app] = output_2
        #print(result_dict[app])
    return result_dict

# This is warp IPC
def hw_ipc_calculate(hw_profile):
    total_cycles = pd.to_numeric(hw_profile['gpc__cycles_elapsed.avg']).sum()
    total_instructions = pd.to_numeric(hw_profile['smsp__inst_executed.sum']).sum()
    return total_instructions / total_cycles

def hw_thread_ipc_calculate(hw_profile):
    cycles = pd.to_numeric(hw_profile['gpc__cycles_elapsed.avg'])
    conversion_factor = pd.to_numeric(hw_profile['smsp__thread_inst_executed_per_inst_executed.ratio'])
    instructions = pd.to_numeric(hw_profile['smsp__inst_executed.sum'])
    total_cycles = 0.0
    total_instructions = 0.0
    for i in range(len(cycles)):
        total_cycles += cycles.loc[i]
        total_instructions += instructions.loc[i] * conversion_factor.loc[i]
    return total_instructions / total_cycles

# This is thread IPC
def sim_ipc_calculate(kernel_list):
    total_cycles = 0.0
    total_instructions = 0.0
    for kernel in kernel_list:
        total_cycles += kernel_list[kernel]['cycles'][-1]
        total_instructions += kernel_list[kernel]['instructions'][-1]
    return total_instructions / total_cycles

# This is thread IPC
def pks_ipc_calculate(kernel_list, sim_table):
    total_instructions = 0.0
    for kernel in kernel_list:
        total_instructions += kernel_list[kernel]['instructions'][-1]
    projected_cycles = sim_table['projected_cycles_pks']
    return total_instructions / projected_cycles

# This is thread IPC
def pka_ipc_calculate(kernel_list, sim_table):
    total_instructions = 0.0
    for kernel in kernel_list:
        total_instructions += kernel_list[kernel]['instructions'][-1]
    projected_cycles = sim_table['projected_cycles_pka']
    return total_instructions / projected_cycles

# This is thread IPC
def pka_ipc_calculate_hw_info(sim_table, hw_profile):
    conversion_factor = pd.to_numeric(hw_profile['smsp__thread_inst_executed_per_inst_executed.ratio'])
    instructions = pd.to_numeric(hw_profile['smsp__inst_executed.sum'])    
    total_instructions = 0.0
    for i in range(len(instructions)):
        total_instructions += instructions.loc[i] * conversion_factor.loc[i]
    projected_cycles = sim_table['projected_cycles_pka']
    return total_instructions / projected_cycles

errors_ = []
speedups_ = []

base_hw_run_directory = '/root/accel-sim-framework/hw_run/device-0/11.2/'
base_sim_run_directory = '/root/accel-sim-framework/sim_run_11.2/'

#benchmarks = ['rodinia-3.1', 'deepbench', 'polybench', 'rodinia_2.0-ft', 'parboil', 'cutlass']
benchmarks = ['rodinia-3.1']
#benchmarks_volta = ['rodinia-3.1', 'deepbench', 'polybench', 'rodinia_2.0-ft', 'parboil', 'cutlass', 'mlperf']
benchmarks_volta = ['rodinia-3.1']
benchmarks_volta_paths = paths.paths(base_hw_run_directory, 'out.csv', benchmarks_volta)
benchmarks_turing_paths = paths.paths(base_hw_run_directory, 'out.csv', benchmarks)
benchmarks_ampere_paths = paths.paths(base_hw_run_directory, 'out.csv', benchmarks)

# ---------- Open HW Profile Data ---------------------------------------------------------------------------------------------------------------- 


RKS_Table_Volta, List_of_RKS_apps_Volta = open_all(benchmarks_volta_paths)
RKS_Best_Choices, RKS_List_of_Errors,RKS_List_of_Speedups, RKS_Best_Choices_Dict, HW_Turing = process_all(RKS_Table_Volta, List_of_RKS_apps_Volta, extra_archs=['turing'], all_paths_turing=benchmarks_turing_paths)
RKS_Best_Choices, RKS_List_of_Errors,RKS_List_of_Speedups, RKS_Best_Choices_Dict, HW_Volta = process_all(RKS_Table_Volta, List_of_RKS_apps_Volta, extra_archs=['turing'], all_paths_turing=benchmarks_volta_paths)

PKS_list, list_of_errors, list_of_speedups, PKS_dict, extra_results  = process_all(RKS_Table_Volta, List_of_RKS_apps_Volta)

paths_turing_simulations = paths.paths(base_sim_run_directory, "/RTX2060-SASS-VISUAL/", benchmarks)
paths_turing_simulations_1B = paths.paths(base_sim_run_directory, "/RTX2060-SASS-VISUAL-1B_INSN/", benchmarks)
paths_volta_simulations = paths.paths(base_sim_run_directory, "/QV100-SASS-VISUAL/", benchmarks)
paths_volta_simulations_1B = paths.paths(base_sim_run_directory, "/QV100-SASS-1B_INSN/", benchmarks)

turing_simulations_full_dict = {}
turing_simulations_1B_dict = {}
volta_simulations_full_dict = {}
volta_simulations_1B_dict = {}

load_from_scratch = True
if(load_from_scratch):
    for app in paths_volta_simulations:
        volta_simulations_full_dict[app] = main(paths_volta_simulations[app], output_log_format_type = 2)
        volta_simulations_1B_dict[app] = main(paths_volta_simulations_1B[app], output_log_format_type = 2)

    for app in paths_turing_simulations:
        # main(path, return_format_type = "dict", output_log_format_type = 2):
        turing_simulations_full_dict[app] = main(paths_turing_simulations[app])
        turing_simulations_1B_dict[app] = main(paths_turing_simulations_1B[app])
else:
    turing_simulations_full_dict = open_variables("Turing-Simulations-Full_dict")
    turing_simulations_1B_dict = open_variables("Turing-Simulations-1B_dict")
    volta_simulations_full_dict = open_variables("Volta-Simulations-Full_dict")
    volta_simulations_1B_dict = open_variables("Volta-Simulations-1B_dict")
    
turing_SIM = process_all_files_discrete_opened_tables(RKS_Best_Choices_Dict, turing_simulations_full_dict)
volta_SIM = process_all_files_discrete_opened_tables(RKS_Best_Choices_Dict, volta_simulations_full_dict)

speedups_geo = []
TBPoint_InterKernel = {}
HW_Profile_Tables = {}
for app in benchmarks_volta_paths:
    print(app)
    try:
        table = open_file(benchmarks_volta_paths[app])
        HW_Profile_Tables[app] = table
    except Exception as e:
        print(e)
        print('error')
        speedups_geo.append(1)
        continue

#    if(len(table) <= 2):
#        print("Too few kernels")
#        speedups_geo.append(1)
#        continue
    #print(len(table))
#    #print(app)
#    try:
#        result = find_best_threshold_value(table)
#    except Exception as e:
#        print(e)
#        print('other error')
#        speedups_geo.append(1)
#        continue
#    speedup = result['result']['total_time']/result['result']['reduced_time']
#    error = (result['result']['projected_time'] - result['result']['total_time'])/result['result']['total_time']
#    TBPoint_InterKernel[app] = {'speedup': speedup, 'error': error, 'selected_kernels': result['result']['selected_kernels'], 'result': result['result']}
#    speedups_geo.append(speedup)


# ---------- Open SIM data ------------------------------------------------------------------------------------------

sim_data = {}
benchmarks = ['rodinia-3.1', 'deepbench', 'polybench', 'parboil', 'cutlass']
paths_benchmark = paths.paths(base_sim_run_directory, "/QV100-SASS-VISUAL/", benchmarks)
for app in paths_benchmark:
    sim_data[app] = main(paths_benchmark[app])

PKA_dict = {}
for app in sim_data:
    try:
        print(app)
        PKA_dict[app] = projection_cycles_only_selected(sim_data[app][0], sim_data[app][1], PKS_dict, window=6, threshold=0.25)
    except Exception as e:
        print(e)
        pass


HW_Turing_Profile_Tables = {}
HW_Ampere_Profile_Tables = {}

for app in benchmarks_ampere_paths:
    print(app)
    try:
        table = open_file(benchmarks_ampere_paths[app])
        HW_Ampere_Profile_Tables[app] = table
    except Exception as e:
        #print(e)
        continue

for app in benchmarks_turing_paths:
    try:
        table = open_file(benchmarks_turing_paths[app])
        HW_Turing_Profile_Tables[app] = table
    except Exception as e:
        #print(e)
        continue

PKS_Turing_dict = {}

for app in HW_Turing_Profile_Tables:
    try:
        cycles_projection = np.sum( [ float(HW_Turing_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) * float(PKS_dict[app]['group_counts'][i]) for i,j in enumerate(PKS_dict[app]['kernel_ids']) ] )
        cycles_reduced = np.sum( [ float(HW_Turing_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) for i,j in enumerate(PKS_dict[app]['kernel_ids']) ] )
        cycles_full = np.sum( [float(HW_Turing_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) for j in range(len(HW_Turing_Profile_Tables[app]))] )
        error = np.abs(cycles_full - cycles_projection) / (cycles_projection)
        speedup = cycles_full / cycles_reduced
        PKS_Turing_dict[app] = {'cycles_projection':cycles_projection, 'cycles_full': cycles_full, 'cycles_reduced': cycles_reduced,'speedup': speedup, 'error': error}
    except Exception as e:
        print(e)
        
PKS_Ampere_dict = {}

for app in HW_Ampere_Profile_Tables:
    try:
        cycles_projection = np.sum( [ float(HW_Ampere_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) * float(PKS_dict[app]['group_counts'][i]) for i,j in enumerate(PKS_dict[app]['kernel_ids']) ] )
        cycles_reduced = np.sum( [ float(HW_Ampere_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) for i,j in enumerate(PKS_dict[app]['kernel_ids']) ] )
        cycles_full = np.sum( [float(HW_Ampere_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) for j in range(len(HW_Ampere_Profile_Tables[app]))] )
        error = np.abs(cycles_full - cycles_projection) / (cycles_projection)
        speedup = cycles_full / cycles_reduced
        PKS_Ampere_dict[app] = {'cycles_projection':cycles_projection, 'cycles_full': cycles_full, 'cycles_reduced': cycles_reduced,'speedup': speedup, 'error': error}
    except Exception as e:
        print(e)
    
PKS_Volta_dict = {}

for app in HW_Profile_Tables:
    try:
        cycles_projection = np.sum( [ float(HW_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) * float(PKS_dict[app]['group_counts'][i]) for i,j in enumerate(PKS_dict[app]['kernel_ids']) ] )
        cycles_reduced = np.sum( [ float(HW_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) for i,j in enumerate(PKS_dict[app]['kernel_ids']) ] )
        cycles_full = np.sum( [float(HW_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[int(j)]) for j in range(len(HW_Profile_Tables[app]))] )
        error = np.abs(cycles_full - cycles_projection) / (cycles_projection)
        speedup = cycles_full / cycles_reduced
        PKS_Volta_dict[app] = {'cycles_projection':cycles_projection, 'cycles_full': cycles_full, 'cycles_reduced': cycles_reduced,'speedup': speedup, 'error': error}
    except Exception as e:
        print(e)

TBPoint_Complete = {}
# PKS_dict
TBPoint_count = 0
for app in sim_data:
    try:
        reduced_cycles = np.sum([tbpoint[app]['results'][str(int(k+1))]['simulated_cycles'] for k in TBPoint_InterKernel[app]['selected_kernels']])
        full_cycles = np.sum([float(sim_data[app][1][k]['total_cycles']) for k in sim_data[app][1]])
        full_instructions = np.sum([float(sim_data[app][0][k]['instructions'][-1]) for k in sim_data[app][0]])
        ipc_full =  full_instructions / full_cycles
        if('group_instructions' in TBPoint_InterKernel[app]['result']):
            TBPoint_count += 1
            group_instructions = [i / HW_Profile_Tables[app]['smsp__thread_inst_executed_per_inst_executed.ratio'].mean() for i in TBPoint_InterKernel[app]['result']['group_instructions']]
            ipc_projection = np.sum([tbpoint[app]['results'][str(int(k+1))]['projected_ipc'] * (full_instructions / group_instructions[i]) for i,k in enumerate(TBPoint_InterKernel[app]['selected_kernels'])])
        else:
            ipc_projection = np.sum([tbpoint[app]['results'][str(int(k+1))]['projected_ipc'] for i,k in enumerate(TBPoint_InterKernel[app]['selected_kernels'])])
    except Exception as e:
        print(app)
        print(e)
        if(sim_data[app] == []):
            continue
        reduced_cycles = np.sum([float(sim_data[app][1][k]['total_cycles']) for k in sim_data[app][1]])
        full_cycles = np.sum([float(sim_data[app][1][k]['total_cycles']) for k in sim_data[app][1]])
        full_instructions = np.sum([float(sim_data[app][0][k]['instructions'][-1]) for k in sim_data[app][0]])
        ipc_full =  full_instructions / full_cycles
        ipc_projection =  ipc_full
    TBPoint_Complete[app] = {'reduced_cycles': reduced_cycles, 'full_cycles': full_cycles, 'ipc_full': ipc_full, 'ipc_projection': ipc_projection}
    

PKA_dict_ = PKA_dict
PKA_Complete = {}
for app in sim_data:
    try:
        print(PKS_dict[app]['kernel_ids'])
        reduced_cycles = np.sum([float(PKA_dict_[app][k]['cycles_reduced']) for k in PKS_dict[app]['kernel_ids']])
        full_cycles = np.sum([float(sim_data[app][1][k]['total_cycles']) for k in sim_data[app][1]])
        cycles_projection = np.sum([float(PKA_dict_[app][k]['cycles_projection']) * float(PKS_dict[app]['group_counts'][i]) for i,k in enumerate(PKS_dict[app]['kernel_ids'])])
        ipc_full =  np.sum([float(sim_data[app][0][k]['instructions'][-1]) for k in sim_data[app][0]]) / full_cycles
        ipc_projection = np.sum([float(sim_data[app][0][k]['instructions'][-1]) for k in sim_data[app][0]]) / cycles_projection
        if(app == 'polybench-syrk'):
            print('wasap')
    except Exception as e:
        if(sim_data[app] == []):
            continue
        print(e)
        full_cycles = np.sum([float(sim_data[app][1][k]['total_cycles']) for k in sim_data[app][1]])
        reduced_cycles = full_cycles
        cycles_projection = full_cycles
        ipc_full = np.sum([float(sim_data[app][0][k]['instructions'][-1]) for k in sim_data[app][0]]) / full_cycles 
        ipc_projection =  ipc_full
    PKA_Complete[app] =  {'reduced_cycles': reduced_cycles, 'full_cycles': full_cycles, 'projected_cycles': cycles_projection, 'ipc_full': ipc_full, 'ipc_projection': ipc_projection}
    
Silicon_Volta_Complete = {}
for app in sim_data:
    try:
        hw_ipc = np.sum([float(HW_Profile_Tables[app]['smsp__thread_inst_executed_per_inst_executed.ratio'].loc[i]) * float(HW_Profile_Tables[app]['smsp__inst_executed.sum'].loc[i]) for i in range(len(HW_Profile_Tables[app]))]) / np.sum([float(HW_Profile_Tables[app]['gpc__cycles_elapsed.avg'].loc[i]) for i in range(len(HW_Profile_Tables[app]))])
    except:
        continue
    Silicon_Volta_Complete[app] = {'ipc': hw_ipc}
    
Simulations1B_Complete = {}
for app in sim_data:
    try:
        instructions = np.sum([ Simulations_1B[app][0][k]['instructions'][-1] for k in Simulations_1B[app][0]])
        cycles = np.sum([Simulations_1B[app][0][k]['cycles'][-1] for k in Simulations_1B[app][0]])
        ipc =  instructions / cycles 
    except Exception as e:
        continue
    Simulations1B_Complete[app] = {'ipc': ipc, 'cycles': cycles, 'instructions': instructions }


save_variables(sim_data, 'simulator_data_10_samples')
#save_variables(TBPoint_Complete, 'tbpoint_complete')
save_variables(PKA_Complete, 'pka_complete')
save_variables(Simulations1B_Complete, '1b_complete')
#save_variables(TBPoint_InterKernel, 'tbpoint_interkernel')
#save_variables(tbpoint, 'tbpoint_intrakernel')
save_variables(PKA_dict, 'pka_intrakernel')
save_variables(PKS_dict, 'pka_interkernel')
save_variables(HW_Profile_Tables, 'HW_Profile_Tables')

save_variables(PKS_Turing_dict, 'PKS_Turing')
save_variables(PKS_Ampere_dict, 'PKS_Ampere')
save_variables(PKS_Volta_dict,  'PKS_Volta')