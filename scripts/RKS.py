import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sys import argv
import gzip
import copy
import json
from pandas import DataFrame
from statsmodels.tsa.arima_model import ARIMA
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from random import randint
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
import random
from scipy import stats
from scipy.spatial import distance

# RKS --------------------------------------------------------------
# Because the nsight compute program doesn't have an exclusive
# export to CSV option, using the buffer stdout instead, we have to
# get rid of some lines, this function does that. 
def ignore_lines(file):
    # check the first 1000 lines, if the phrase "device__attribute_async_engine_count"
    # is found, else return -99
    # This phrase is always printed by nsight-compute.
    i = 0
    with open(file) as in_file:
        for line in in_file:
            if ("device__attribute_async_engine_count" in line):
                return i
            elif(i > 1000):
                return -99
            i += 1
            
# General open file function, takes in a nsight-compute csv filename, 
# returns a pandas table.
def open_file(filename):
    #print(filename)
    lines_to_ignore = ignore_lines(filename)
    if(lines_to_ignore == -99):
        lines_to_ignore = 0
    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)
    if(table.loc[0,'ID'] != 0.0):
        table = table.drop(0)
        table.index = range(len(table))
    # For certain systems nsight-compute prints
    # comma-deliniated numbers, e.g. 1,000 instead of 1000.
    # We need to get rid of that. 
    table = table.replace(',','', regex=True)
    return table

# List of agnostic features to take into consideration when performing
# the PCA (i.e. only these variables are going to be used)
agnostic_features = ['l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum',
                     'l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum',
                     #'l1tex__t_sectors_pipe_lsu_mem_local_op_ld.sum',
                     'smsp__inst_executed.sum','smsp__thread_inst_executed_per_inst_executed.ratio',
                     #'smsp__sass_inst_executed_op_global_atom.sum',
                     'smsp__inst_executed_op_global_ld.sum',
                     'smsp__inst_executed_op_global_st.sum',
                     'smsp__inst_executed_op_shared_ld.sum',
                     'smsp__inst_executed_op_shared_st.sum',
                     #'smsp__inst_executed_op_surface_atom.sum',
                     #'smsp__inst_executed_op_surface_ld.sum',
                     #'smsp__inst_executed_op_surface_red.sum',
                     #'smsp__inst_executed_op_surface_st.sum',
                     'sass__inst_executed_global_loads',
                     'sass__inst_executed_global_stores',
                     'launch__grid_size']

# Opens the cycles-profiled-only csvs generated by
# the nsight-compute utility, obtained using the 
# run_hw.py under the accel-sim utility.
def open_cycles(filename):
    #print(filename)
    lines_to_ignore = ignore_cycle_lines(filename)
    if(lines_to_ignore == -99):
        lines_to_ignore = 0
    table = pd.read_csv(filename, skiprows=lines_to_ignore,low_memory=False)
    return table

def ignore_cycle_lines(file):
    # check the first 1000 lines, if the phrase "Section Name" is not found
    # return -99. Same as ignore_lines above, but for the cycles-only version.
    i = 0
    with open(file) as in_file:
        for line in in_file:
            if ("Section Name" in line):
                return i
            elif(i > 1000):
                return -99
            i += 1

def generate_DF(table, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID']):
    x_large = table.loc[:,agnostic_features].values
    x_large = StandardScaler().fit_transform(x_large)
    temp = table.loc[:,agnostic_features]
    if(True):
        for value in agnostic_features:
            if(temp[value].isnull().values.any()):
                print(value)
    for new_col in appendable_columns:
        temp = table[[new_col]]
        if "Name" not in new_col:
            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))
        x_large = pd.concat([x_large, temp], axis = 1)
    return x_large

def generate_PCA(table, pca_variation=0.9995, appendable_columns=['Kernel Name', 'gpc__cycles_elapsed.avg', 'ID'], debug=False):
    x_large = table.loc[:,agnostic_features].values
    #x_large = x_large.apply(lambda x: pd.to_numeric(x.astype(str).str.replace(',','')))
    #x_large = x_large.values
    x_large = StandardScaler().fit_transform(x_large)
    temp = table.loc[:,agnostic_features]
    if(True):
        for value in agnostic_features:
            if(temp[value].isnull().values.any()):
                print(value)
    pca_components = pca_variation
    pca = PCA(pca_components)
    principalComponents_Large = pca.fit_transform(x_large)
    principalDf_Large = pd.DataFrame(data = principalComponents_Large, columns = ['principal component '+str(x+1) for x in range(pca.n_components_)])
    for new_col in appendable_columns:
        temp = table[[new_col]]
        if "Name" not in new_col:
            temp = pd.to_numeric(pd.Series( table.loc[:,new_col] ))
        principalDf_Large = pd.concat([principalDf_Large, temp], axis = 1)
    #finalDf_Large = pd.concat([finalDf_Large,truncated_names], axis = 1)
    #finalDf_Large = pd.concat([finalDf_Large, nsight_csv[['ID']]], axis = 1)
    #finalDf_sorted = finalDf_Large#.sort_values('sm__inst_issued.avg.per_cycle_active [inst/cycle]')
    #%matplotlib inline
    #fig = plt.figure(figsize = (10,10))
    #ax = fig.add_subplot(111) 
    #ax.set_title('PCA components - variance', fontsize = 20)
    #plt.bar(range(pca.n_components_), pca.explained_variance_ratio_, color='black')
    #plt.xlabel('PCA features')
    #plt.ylabel('variance %')
    #plt.xticks(range(pca.n_components_))
    #plt.show()
    # Clustering
    #k_means = KMeans(n_clusters=2, random_state=2).fit(principalComponents_Large)
    #finalDf_sorted['Segments'] = k_means.labels_
    #print(principalDf_Large)
    return principalDf_Large, principalComponents_Large 

# Naively choosing one from each group at random
def kmeans_clustering(dataFrame,principalComponents_dataFrame,starting_from=1, ending_at=20, column_variable='gpc__cycles_elapsed.avg',print_output=False):
    total_runtime = dataFrame[column_variable].sum()
    complete_groups = []
    if(ending_at > len(dataFrame)):
        ending_at = len(dataFrame)+1
    results = {'random_choices_projections':[], 'random_choices_vectors':[], 'random_choices_names':[], 
               'random_choices_id':[], 'random_choices_speedups':[],'first_choices_projections':[],
               'center_choices_projections':[], 'center_choices_vectors':[], 'center_choices_names':[],
               'center_choices_id':[], 'center_choices_speedups':[], 'center_choices_errors': [],
               'complete_groups':[],
               'first_choices_vectors':[], 'first_choices_names':[], 'first_choices_id':[], 'group_count':[], 
               'total_runtime':total_runtime, 'group_number': [], 'errors': [], 'speedups': [], 'number_of_kernels': []}
    for i in range(starting_from, ending_at):
        random_choices = [] # Randomly select a kernel from the group
        random_choices_name = []
        random_choices_id = []
        mean_choices = []   # Select the kernel with the value closest to the mean
        max_choices = []    # Select the largest kernel value
        first_choices = []   # Selects the first chronological value
        first_choices_name = []
        first_choices_id = []
        center_choices_id = []
        closest_to_mean = []
        closest_to_mean_names = []
        group_count = []    # Number of elements inside the cluster i
        complete_groups_df = []
        k_means = KMeans(n_clusters=i, random_state=4).fit(principalComponents_dataFrame)
        dataFrame['Segments'] = k_means.labels_
        center_ids = cluster_centers(dataFrame,principalComponents_dataFrame,k_means)
        for group in np.unique(k_means.labels_):
            temp_df = dataFrame.loc[dataFrame['Segments'] == group]
            #closest_to_mean.append(temp_df.loc[center_ids[group], column_variable])
            #closest_to_mean_names.append(temp_df.loc[center_ids[group], 'Kernel Name'])
            complete_groups_df.append(temp_df)
            temp_df.index = range(len(temp_df))
            #value_first = temp_df.loc[0,column_variable]
            first_choices.append(temp_df.loc[0,column_variable])
            first_choices_name.append(temp_df.loc[0,'Kernel Name'])
            first_choices_id.append(temp_df.loc[0,'ID'])
            #center_choices_id.append(center_ids[group])
            temp_df_sorted = temp_df.sort_values(column_variable)
            temp_df_sorted.index = range(len(temp_df_sorted))
            group_count.append(len(temp_df))
            random_choice = random.randint(0,len(temp_df))
            random_choice_name = ''
            max_choice = temp_df_sorted.loc[len(temp_df)-1, column_variable]
            try:
                value_mean = temp_df_sorted.loc[int(len(temp_df)/2),column_variable]
            except:
                print(int(len(temp_df)/2))
            try:
                value_random = temp_df_sorted.loc[random_choice,column_variable]
                random_choice_name = temp_df_sorted.loc[random_choice, 'Kernel Name']
            except:
                value_random = temp_df_sorted.loc[0,column_variable]
                random_choice_name = temp_df_sorted.loc[0,'Kernel Name']
            random_choices.append(value_random)
            random_choices_id.append(random_choice)
            random_choices_name.append(random_choice_name)
            mean_choices.append(value_mean)
            max_choices.append(max_choice)
        complete_groups.append(complete_groups_df)
        random_runtime = [random_choices[i] * group_count[i] for i in range(len(random_choices))]
        mean_runtime = [mean_choices[i] * group_count[i] for i in range(len(random_choices))]
        max_runtime = [max_choices[i] * group_count[i] for i in range(len(random_choices))]
        first_runtime = [first_choices[i] * group_count[i] for i in range(len(random_choices))]
        #closest_runtime = [closest_to_mean[i] * group_count[i] for i in range(len(random_choices))]
        
        if(print_output):
            print('For '+str(i)+ ' groups')
            print('----------------------------------------------')
            print(dataFrame[column_variable])
            print('The actual run time is: '+str(total_runtime))
            print('----------------- Projections ----------------')
            print(' ')
            print('The random value run time is: '+str(np.sum(random_runtime)))
            print('The error is '+ str(np.sum(random_runtime) / total_runtime ))
            print('The mean value run time is: '+str(np.sum(mean_runtime)))
            print('The error is '+ str(np.sum(mean_runtime) / total_runtime ))
            print('The max value run time is: '+str(np.sum(max_runtime)))
            print('The error is '+ str(np.sum(max_runtime) / total_runtime ))
            print('The first value run time is: '+str(np.sum(first_runtime)))
            print('The error is '+ str(np.sum(first_runtime) / total_runtime ))
            print('The speedup is '+ str(total_runtime / np.sum(first_choices)))
            print('The reduced number of kernels '+str(len(first_runtime)))
            print('The total number of kernels '+str(len(dataFrame)))
            print('The first choice vector is '+str(first_choices))
            print('The number of elements per group is '+str(group_count))
            print('The names of the first choices are '+str(first_choices_name))
            print('The kernel IDs of the first choices are '+str(first_choices_id))
        #print('Their product is '+str( [first_choices[i] * group_count[i] for i in range(len(first_choices))] ))
            print(' ')
        results['errors'].append(np.abs((np.sum(first_runtime)-total_runtime)) / total_runtime)
        results['speedups'].append(total_runtime / np.sum(first_choices))
        results['first_choices_vectors'].append(first_choices)
        results['first_choices_projections'].append(first_runtime)
        results['first_choices_names'].append(first_choices_name)
        results['first_choices_id'].append(first_choices_id)
        results['random_choices_vectors'].append(random_choices)
        results['random_choices_projections'].append(random_runtime)
        results['random_choices_names'].append(random_choices_name)
        results['random_choices_id'].append(random_choices_id)
        #results['center_choices_projections'].append(closest_runtime)
        #results['center_choices_id'].append(center_choices_id)
        #results['center_choices_vectors'].append(closest_to_mean)
        #results['center_choices_names'].append(closest_to_mean_names)
        #results['center_choices_errors'].append(np.abs((total_runtime - np.sum(closest_runtime))) / total_runtime)
        results['group_count'].append(group_count)
        results['group_number'].append(group)
        results['complete_groups'].append(complete_groups)
        results['number_of_kernels'].append(len(dataFrame))
        #print(finalDf_sorted.loc[finalDf_sorted['Segments'] == group])
    return results
#kmeans_clustering(finalDf_sorted, principalComponents_Large)

# Returns the ID of the element closest to the group's centers
def cluster_centers(dataFrame, principalComponents_dataFrame, k_means, debug=False):
    centers_vector = []
    centers_ids = []
    clusters_centers = k_means.cluster_centers_
    temp_pca_df = dataFrame.copy()
    #labels = ['principal component '+str(x+1) for x in range(len(np.unique(k_means.labels_)))]
    for group in np.unique(k_means.labels_):
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_grouped_pca_df = temp_df.filter(regex='principal')
        original_indeces = temp_grouped_pca_df.index.values.tolist()
        temp_grouped_pca_df.index = range(len(temp_grouped_pca_df))
        minimum_distance = temp_grouped_pca_df.loc[0,:].values.tolist()
        minimum_index = original_indeces[0]
        #print(minimum_distance)
        if(debug):
            print('The clusters centers are: '+str(clusters_centers[group]))
            print('The first data point is : '+str(minimum_distance))
        minimum_distance = distance.euclidean(minimum_distance, clusters_centers[group])
        if(debug):
            print(minimum_distance)
        for i in range(len(temp_grouped_pca_df)):
            data_point = temp_grouped_pca_df.loc[i,:].values.tolist()
            temp_distance = distance.euclidean(data_point, clusters_centers[group])
            if(temp_distance < minimum_distance):
                minimum_distance = temp_distance
                minimum_index = original_indeces[i]
        if(debug):
            print(minimum_distance, minimum_index)
        centers_ids.append(minimum_index)
    if(debug):
        print('Returns from function \nNew iteration\n\n')
    return centers_ids

def point_closest_to_all(dataFrame, principalComponents_dataFrame, k_means, debug=False):
    closest_ids = []
    for group in np.unique(k_means.labels_):
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_grouped_pca_df = temp_df.filter(regex='principal')
        original_indeces = temp_grouped_pca_df.index.value.tolist()
    
def check_results(dataFrame, best_choices, group_counts, column_variable='gpc__cycles_elapsed.avg',print_debug=False):
    total_runtime = pd.to_numeric(dataFrame[column_variable]).sum()
    projected_runtime = 0.0
    for i in range(len(best_choices)):
        index = int(best_choices[i])
        projected_runtime += float(dataFrame.loc[index, column_variable]) * group_counts[i]
    if(print_debug):
        print('The actual runtime is: '+str(total_runtime))
        print('The projected runtime with the borrowed K-Means is: '+str(projected_runtime))
        print('The ratio is '+str(projected_runtime/total_runtime))
    return [total_runtime, projected_runtime, np.abs(projected_runtime - total_runtime)/total_runtime]

def open_all(profile_paths):
    Beeg_Table = {}
    list_of_apps = []
    for app in profile_paths:
        try:
            print(app)
            table = open_file(profile_paths[app])
            DF_Large, PC_Large = generate_PCA(table)
            results = kmeans_clustering(DF_Large, PC_Large)
            Beeg_Table[app] = results
            list_of_apps.append(app)
        except:
            print(app)
            print("Has a problem")
    return Beeg_Table, list_of_apps

def process_all(Beeg_Table, list_of_apps, extra_archs=[''], min_error_arg = 0.05, all_paths_ampere = [], all_paths_turing = []):
    list_of_speedups = []
    list_of_errors = []
    best_choices_dict = {}
    dict_best_choices = {}
    for app in list_of_apps:
        min_error = 99
        index_min_error = 99
        min_error_speedup = 99
        group_counts_min = []
        kernel_ids_min = []
        first_error_less_than_4_percent = []
        first_error_less_than_3_percent = []
        first_error_less_than_2_percent = []
        for i in range(len(Beeg_Table[app]['errors'])):
            error = Beeg_Table[app]['errors'][i]
            #error = Beeg_Table[app]['center_choices_errors'][i]
            speedup = Beeg_Table[app]['speedups'][i]
            group_counts = Beeg_Table[app]['group_count'][i]
            #kernel_ids = Beeg_Table[app]['center_choices_id'][i]
            kernel_ids = Beeg_Table[app]['first_choices_id'][i]
            print("The error is: "+str(error))
            print("The speedup is: "+str(speedup))
            if(error < min_error):
                min_error = error
                index_min_error = i
                min_error_speedup = speedup
                group_counts_min = group_counts
                kernel_ids_min = kernel_ids
            if(error < min_error_arg):
                first_error_less_than_4_percent.append([error,i,speedup, group_counts, kernel_ids])
                if(error < min_error_arg/2.0):
                    first_error_less_than_3_percent.append([error,i,speedup])
                    if(error < min_error_arg/4.0):
                        first_error_less_than_2_percent.append([error,i,speedup])
        if(len(first_error_less_than_4_percent)==0):
            first_error_less_than_4_percent.append([min_error, index_min_error, min_error_speedup,group_counts_min, kernel_ids_min])
        list_of_speedups.append(first_error_less_than_4_percent[0][2])
        list_of_errors.append(first_error_less_than_4_percent[0][0])
        best_choices_dict[app] = [first_error_less_than_4_percent[0][4],first_error_less_than_4_percent[0][3],first_error_less_than_4_percent[0][2]]
        dict_best_choices[app] = {'kernel_ids':first_error_less_than_4_percent[0][4], 'group_counts': first_error_less_than_4_percent[0][3], 'speedup': first_error_less_than_4_percent[0][2], 'error': first_error_less_than_4_percent[0][0]}
        #print(best_choices_dict[app])

    extra_archs_results = {}
    if('turing' in extra_archs):
        list_of_turing_apps = []
        list_of_turing_results = []
        list_of_turing_speedups = []
        list_of_turing_names = []
        for app in all_paths_turing:
            table = open_file(all_paths_turing[app])
            try:
                DF_Large = generate_DF(table)
                list_of_turing_apps.append(app)
                temp_results = check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1])
                list_of_turing_results.append(temp_results)
                list_of_turing_speedups.append(temp_results[0]/np.sum(best_choices_dict[app][0]))
                list_of_turing_names.append(app)
            except:
                pass
        extra_archs_results['turing'] = {'results': list_of_turing_results, 'speedups': list_of_turing_speedups, 'names': list_of_turing_names}
            #print(check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1]))
            
    if('ampere' in extra_archs):
        list_of_ampere_apps = []
        list_of_ampere_results = []
        list_of_ampere_speedups = []
        list_of_ampere_names = []
        for app in all_paths_ampere:
            table = open_file(all_paths_ampere[app])
            try:
                DF_Large = generate_DF(table)
                list_of_ampere_apps.append(app)
                temp_results = check_results(DF_Large,best_choices=best_choices_dict[app][0], group_counts=best_choices_dict[app][1])
                list_of_ampere_results.append(temp_results)
                list_of_ampere_speedups.append(temp_results[0]/np.sum(best_choices_dict[app][0]))
                list_of_ampere_names.append(app)
            except:
                pass
        extra_archs_results['ampere'] = {'results': list_of_ampere_results, 'speedups': list_of_ampere_speedups, 'names': list_of_ampere_names}

    return best_choices_dict, list_of_errors, list_of_speedups, dict_best_choices, extra_archs_results

def kernel_distribution_per_kmean_groups(dataFrame,principalComponents_dataFrame,best_choice=5, kernel_name = 'Kernel Name'):
    #for i in range(starting_from, ending_at):
    histogram_cu = {}
    histogram_fn = {}
    total_runtimes = []
    k_means = KMeans(n_clusters=best_choice, random_state=4).fit(principalComponents_dataFrame)
    dataFrame['Segments'] = k_means.labels_
    print(np.unique(k_means.labels_))
    unique_cufunction_names = dataFrame[kernel_name].unique()
    unique_kernel_names = dataFrame[kernel_name].unique()
    for group in np.unique(k_means.labels_):
        for key in unique_cufunction_names:
            histogram_cu[key] = 0
        for key in unique_kernel_names:
            histogram_fn[key] = 0
        # Extract only the kernels related to the group inside the loop
        fig = plt.figure(figsize = (20,10))
        temp_df = dataFrame.loc[dataFrame['Segments'] == group]
        temp_df.index = range(len(temp_df))
        runtime = temp_df['gpc__cycles_elapsed.avg'].sum()
        total_count = len(temp_df)
        for i in range(len(temp_df)):
            cu_name = temp_df.loc[i, kernel_name]
            func_name = temp_df.loc[i, kernel_name]
            histogram_cu[cu_name] += 1
            histogram_fn[func_name] += 1
        total_runtimes.append(runtime)
        ax = fig.add_subplot(121)
        ax.set_title('K-Means Group '+str(group),fontsize=30)
        chart = plt.bar(histogram_cu.keys(), histogram_cu.values(), 1)
        ax.legend(['Kernel count: '+str(total_count)+'\nTotal Runtime: '+f"{runtime:,}"], fontsize=20)
        ax.set_xticklabels(histogram_cu.keys(),rotation=90)
        ax = fig.add_subplot(122)
        chart = plt.bar(histogram_fn.keys(), histogram_fn.values(), 1)
        ax.set_xticklabels(histogram_fn.keys(),rotation=90)
    fig = plt.figure(figsize = (20,10))
    ax = fig.add_subplot(111)
    ax.set_title("Total Runtimes as a function of Group Id", fontsize=30)
    plt.bar(range(len(total_runtimes)), total_runtimes)
    plt.xlabel('Group ID',fontsize=20)
    plt.ylabel('Run time',fontsize=20)
        #temp_df['CUfunction'].hist()

#kernel_distribution_per_kmean_groups(finalDf_sorted, principalComponents_Large, 14, 5)

def TBPoint(dataFrame):
    # We need to choose the 4 metrics
    # sm__inst_executed.sum
    # smsp__thread_inst_executed.sum
    # l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum
    # l1tex__t_requests_pipe_lsu_mem_global_op_st.sum
    # launch_size
    tbpoint_features = ['sm__inst_executed.sum', 'smsp__thread_inst_executed.sum', 
    'l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum', 'l1tex__t_requests_pipe_lsu_mem_global_op_st.sum','launch__grid_size']
    x_large = dataFrame.loc[:,tbpoint_features].values
    clustering = AgglomerativeClustering().fit(x_large)
    print(clustering.labels_)
    dataFrame['Segments'] = clustering.labels_
    selected_kernels = []
    for group in np.unique(clustering.labels_):
        print("banana")